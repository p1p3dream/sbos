{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8286c-fb55-4a56-9a4a-959269a02ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####CREATE TICKER SPECIFIC FILES\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import glob\n",
    "import gzip\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path, ticker, year_month, output_dir):\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        df = pl.read_csv(f)\n",
    "    df_filtered = df.filter(df['ticker'] == ticker)\n",
    "    output_file = os.path.join(output_dir, f\"{year_month}_{ticker}.csv\")\n",
    "    # Since writing to the same file from multiple threads is not safe, we return the filtered DataFrame\n",
    "    return df_filtered, output_file\n",
    "\n",
    "# Specify the directory containing the data files and other parameters\n",
    "data_dir = \"/Users/brandon/Documents/polygon_data/minute_aggs/\"\n",
    "tickers = [\"NVDA\",\"MSFT\", \"TSM\", \"AAPL\",\"FICO\"]  # List of tickers to process\n",
    "tickers = [\"CRWD\", \"NDVX\", \"GOOG\", \"META\"]  # List of tickers to process\n",
    "\n",
    "# Configure the maximum number of threads to use\n",
    "max_workers = 24\n",
    "\n",
    "# Loop through each ticker\n",
    "for ticker in tickers:\n",
    "    output_dir = f\"/Users/brandon/Documents/stonk_bot_data/{ticker}/\" \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a dictionary to store monthly dataframes\n",
    "    monthly_dfs = {}\n",
    "\n",
    "    # Loop through each year from 2019 to 2025\n",
    "    for year in range(2024, 2025):\n",
    "        for month in range(1, 13):\n",
    "            month_dir = os.path.join(data_dir, str(year), \"{:02d}\".format(month))\n",
    "            if not os.path.exists(month_dir):\n",
    "                print(f\"No data found for {year}-{month:02d}\")\n",
    "                continue\n",
    "\n",
    "            # Collect all file paths to process\n",
    "            file_paths = [os.path.join(month_dir, file) for file in os.listdir(month_dir) if file.endswith(\".csv.gz\")]\n",
    "\n",
    "            # Use ThreadPoolExecutor to process files in parallel\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Create a future for each file\n",
    "                futures = [executor.submit(process_file, file_path, ticker, f\"{year}-{month:02d}\", output_dir) for file_path in file_paths]\n",
    "\n",
    "                # As futures complete, process their results\n",
    "                for future in as_completed(futures):\n",
    "                    df_filtered, output_file = future.result()\n",
    "                    # Append or extend the DataFrame in monthly_dfs\n",
    "                    year_month = os.path.basename(output_file).replace(f\"_{ticker}.csv\", \"\")\n",
    "                    if year_month not in monthly_dfs:\n",
    "                        monthly_dfs[year_month] = df_filtered\n",
    "                    else:\n",
    "                        monthly_dfs[year_month] = monthly_dfs[year_month].extend(df_filtered)\n",
    "                    print(f\"Processed {output_file}\")\n",
    "\n",
    "    # Output the combined monthly DataFrames to files\n",
    "    for year_month, df in monthly_dfs.items():\n",
    "        output_file = os.path.join(output_dir, f\"{year_month}_{ticker}.csv\")\n",
    "        df.write_csv(output_file)\n",
    "        print(f\"Saved {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f51b6-2ab0-40e1-873a-678b6f67c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******CREATE BASE DATA FRAME****************\n",
    "\n",
    "import os\n",
    "import os\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "import numpy as np\n",
    "from polars import Config\n",
    "import pytz\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "pl.Config.set_tbl_rows(1000)\n",
    "pl.Config(tbl_cols=-1)\n",
    "\n",
    "tickers = [\"NVDA\"]\n",
    "# tickers = ['NVDA', 'MSFT', 'AAPL', 'TSM']\n",
    "# tickers = ['NVDA', 'MSFT', 'AAPL', 'TSM',\n",
    "#            'GOOG', 'FICO']\n",
    "# tickers = [\"NVDA\", \"AAPL\", \"GOOG\", \"MSFT\",\n",
    "#            \"TSM\", \"CRWD\", \"NVDX\", \"FICO\"]  # List of tickers\n",
    "\n",
    "# directory_path = f\"/Users/brandon/Documents/stonk_bot_data/{ticker}/\"\n",
    "\n",
    "def combine_data_for_tickers(tickers):\n",
    "    combined_df = None  # Initialize combined DataFrame\n",
    "\n",
    "    for ticker in tickers:\n",
    "        directory_path = f\"/Users/brandon/Documents/stonk_bot_data/{ticker}/\"\n",
    "\n",
    "        try:\n",
    "            # List all CSV files in the directory\n",
    "            files = [os.path.join(directory_path, file) for file in os.listdir(directory_path)\n",
    "                     if file.endswith(\".csv\")]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found for ticker: {ticker}. Skipping...\")\n",
    "            continue  # Skip to the next ticker\n",
    "\n",
    "        # Ensure there are files to process\n",
    "        if not files:\n",
    "            print(f\"No CSV files found for ticker: {ticker}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Read the first CSV file to infer the schema\n",
    "        df = pl.read_csv(files[0])\n",
    "        schema = {col: df[col].dtype for col in df.columns}\n",
    "\n",
    "        # Create an empty list to store DataFrames\n",
    "        dfs = []\n",
    "\n",
    "        # Read each CSV file and append its DataFrame to the list, applying the schema\n",
    "        for file in files:\n",
    "            df = pl.read_csv(file, dtypes=schema)\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Concatenate all DataFrames into a single DataFrame for the current ticker\n",
    "        if dfs:  # Ensure dfs is not empty\n",
    "            ticker_df = pl.concat(dfs)\n",
    "            ticker_df = ticker_df.with_columns(pl.lit(ticker).alias('ticker'))\n",
    "\n",
    "            # Append the DataFrame for the current ticker to the combined DataFrame\n",
    "            if combined_df is None:\n",
    "                combined_df = ticker_df\n",
    "            else:\n",
    "                combined_df = combined_df.vstack(ticker_df)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "df = combine_data_for_tickers(tickers)\n",
    "\n",
    "\n",
    "\n",
    "# Drop index column if it exists\n",
    "if 'index' in df.columns:\n",
    "    df = df.drop('index')\n",
    "\n",
    "# Drop duplicates\n",
    "df = df.unique(subset=df.columns)\n",
    "\n",
    "\n",
    "# eastern_tz = ZoneInfo(\"America/New_York\")\n",
    "\n",
    "# def convert_to_est(df):\n",
    "#     # Convert nanoseconds to microseconds (Polars' timestamp precision)\n",
    "#     df = df.with_columns([\n",
    "#         (pl.col('window_start') // 1000).cast(pl.Datetime('us')).alias('window_start_utc'),\n",
    "#         ((pl.col('window_start') + 60 * 1e9) // 1000).cast(pl.Datetime('us')).alias('window_end_utc')\n",
    "#     ])\n",
    "\n",
    "#     # Convert UTC to Eastern Time\n",
    "#     df = df.with_columns([\n",
    "#         pl.col('window_start_utc').dt.replace_time_zone(\"UTC\").dt.convert_time_zone(eastern_tz).alias('window_start_est'),\n",
    "#         pl.col('window_end_utc').dt.replace_time_zone(\"UTC\").dt.convert_time_zone(eastern_tz).alias('window_end_est')\n",
    "#     ])\n",
    "\n",
    "#     return df\n",
    "# df = agg_df\n",
    "\n",
    "# Ensure Polars DataFrame contains the data with nanoseconds\n",
    "df = df.with_columns([\n",
    "    (pl.col(\"window_start\").alias(\"window_start_ts\")),\n",
    "    (pl.col(\"window_start\") / 1e3).cast(pl.Datetime).alias(\"window_start_dt\"),\n",
    "    ((pl.col(\"window_start\") + 60 * 1e9) / 1e3).cast(pl.Datetime).alias(\"window_end_dt\")\n",
    "])\n",
    "# # Convert UTC datetime to Eastern Time\n",
    "# df = df.with_columns([\n",
    "#     pl.col(\"window_start_dt\").dt.convert_time_zone(\"US/Eastern\").alias(\"window_start_est\"),\n",
    "#     pl.col(\"window_end_dt\").dt.convert_time_zone(\"US/Eastern\").alias(\"window_end_est\")\n",
    "# ])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"window_start_dt\").dt.replace_time_zone(\"UTC\").alias(\"window_start_dt_utc\"),\n",
    "    pl.col(\"window_end_dt\").dt.replace_time_zone(\"UTC\").alias(\"window_end_dt_utc\")\n",
    "])\n",
    "\n",
    "# Convert the timezone to \"US/Eastern\"\n",
    "df = df.with_columns([\n",
    "    pl.col(\"window_start_dt_utc\").dt.convert_time_zone(\"US/Eastern\").alias(\"window_start_est\"),\n",
    "    pl.col(\"window_end_dt_utc\").dt.convert_time_zone(\"US/Eastern\").alias(\"window_end_est\")\n",
    "])\n",
    "\n",
    "# Define trading hours\n",
    "pre_market_start = dt.time(4, 0)  # Pre-market starts at 4:00 AM\n",
    "trading_start = dt.time(9, 30)\n",
    "trading_end = dt.time(16, 0)\n",
    "\n",
    "# Create flags for regular trading hours, after hours, and pre-market hours\n",
    "regular_trading_hours = pl.when(\n",
    "    (pl.col('window_start_est').dt.time() >= trading_start) &\n",
    "    (pl.col('window_start_est').dt.time() <= trading_end)\n",
    ").then(1).otherwise(0)\n",
    "\n",
    "after_hours = pl.when(\n",
    "    (pl.col('window_start_est').dt.time() > trading_end) |\n",
    "    (pl.col('window_start_est').dt.time() < pre_market_start)\n",
    ").then(1).otherwise(0)\n",
    "\n",
    "pre_market_hours = pl.when(\n",
    "    (pl.col('window_start_est').dt.time() >= pre_market_start) &\n",
    "    (pl.col('window_start_est').dt.time() < trading_start)\n",
    ").then(1).otherwise(0)\n",
    "\n",
    "# Add the new columns to the DataFrame\n",
    "df = df.with_columns([\n",
    "    regular_trading_hours.alias('regular_trading_hours'),\n",
    "    after_hours.alias('after_hours'),\n",
    "    pre_market_hours.alias('pre_market_hours'),\n",
    "    df['window_start_est'].dt.date().alias('transaction_date')\n",
    "])\n",
    "\n",
    "unfiltered_df = df\n",
    "df = df.filter(pl.col('regular_trading_hours') == 1)\n",
    "print(f\"Length unfiltered {len(unfiltered_df)}\")\n",
    "print(f\"Length filtered {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c39aa-ac7f-4bad-9420-ca76819cf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436b3e7-9a04-45b1-9708-6065950a43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Apply Stock Splits\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "import datetime as dt\n",
    "import polars as pl\n",
    "\n",
    "pl.Config.set_tbl_formatting(\"UTF8_FULL\")\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "\n",
    "\n",
    "def get_stock_splits(ticker):\n",
    "    url = f\"https://api.polygon.io/v3/reference/splits?ticker={ticker}&apiKey={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    splits = response.json().get(\"results\", [])\n",
    "    return splits\n",
    "\n",
    "# List of tickers\n",
    "tickers = [\"NVDA\",\"MSFT\", \"TSM\", \"AAPL\",\"FICO\", \"CRWD\", \"NDVX\", \"GOOG\", \"META\"]\n",
    "\n",
    "# Combine splits data for all tickers\n",
    "def combine_splits_for_tickers(tickers):\n",
    "    all_splits = []\n",
    "    for ticker in tickers:\n",
    "        splits = get_stock_splits(ticker)\n",
    "        for split in splits:\n",
    "            all_splits.append({\n",
    "                \"ticker\": split['ticker'],\n",
    "                \"execution_date\": split['execution_date'],\n",
    "                \"split_from\": split['split_from'],\n",
    "                \"split_to\": split['split_to']\n",
    "            })\n",
    "    \n",
    "    # Convert to Polars DataFrame\n",
    "    splits_df = pl.DataFrame(all_splits)\n",
    "    return splits_df\n",
    "\n",
    "# Fetch and combine splits for all tickers\n",
    "splits_df = combine_splits_for_tickers(tickers)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Check the type of the execution_date column\n",
    "if splits_df['execution_date'].dtype == pl.Date:\n",
    "    print(\"execution_date is already a Date type. No conversion needed.\")\n",
    "elif splits_df['execution_date'].dtype == pl.Datetime:\n",
    "    # If it's a datetime, convert to date\n",
    "    splits_df = splits_df.with_columns([\n",
    "        pl.col('execution_date').dt.date().alias('execution_date')\n",
    "    ])\n",
    "elif splits_df['execution_date'].dtype == pl.Utf8:\n",
    "    # If it's a string, convert to date\n",
    "    splits_df = splits_df.with_columns([\n",
    "        pl.col('execution_date').str.strptime(pl.Date, \"%Y-%m-%d\").alias('execution_date')\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected type for execution_date: {splits_df['execution_date'].dtype}\")\n",
    "\n",
    "\n",
    "def apply_adjustments(df: pl.DataFrame, splits_df: pl.DataFrame, type: str, price_cols: list, volume_cols: list) -> pl.DataFrame:\n",
    "    # Ensure splits_df is sorted by execution_date\n",
    "    splits_df = splits_df.sort('execution_date')\n",
    "\n",
    "    # Convert window_start_est to date\n",
    "    df = df.with_columns([\n",
    "        pl.col('window_start_est').dt.date().alias(\"window_start_date\")\n",
    "    ])\n",
    "\n",
    "    # Determine the prefix based on the type\n",
    "    prefix = \"adjusted_\" if type == \"stock\" else \"adjusted_option_\"\n",
    "\n",
    "    # Initialize the applied_splits column as an empty list\n",
    "    df = df.with_columns([pl.lit(\"\").alias('applied_splits')])\n",
    "\n",
    "    # Create adjusted columns for price and volume\n",
    "    for col in price_cols + volume_cols:\n",
    "        df = df.with_columns([pl.col(col).alias(f'{prefix}{col}')])\n",
    "\n",
    "    for split in splits_df.to_dicts():\n",
    "        ticker = split['ticker']\n",
    "        execution_date = split['execution_date']\n",
    "        # If execution_date is already a date, we don't need to parse it\n",
    "        if isinstance(execution_date, dt.date):\n",
    "            pass\n",
    "        elif isinstance(execution_date, str):\n",
    "            execution_date = dt.datetime.strptime(execution_date, '%Y-%m-%d').date()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type for execution_date: {type(execution_date)}\")\n",
    "        \n",
    "        split_from = split['split_from']\n",
    "        split_to = split['split_to']\n",
    "        ratio = split_to / split_from\n",
    "\n",
    "        # Convert the split dictionary to a string for storage\n",
    "        split_str = str(split)\n",
    "\n",
    "        # Apply split to relevant rows\n",
    "        adjustments = []\n",
    "        for col in price_cols:\n",
    "            adjustments.append(\n",
    "                pl.when((pl.col('ticker') == ticker) & (pl.col('window_start_date') < execution_date))\n",
    "                .then(pl.col(f'{prefix}{col}') / ratio)\n",
    "                .otherwise(pl.col(f'{prefix}{col}')).alias(f'{prefix}{col}')\n",
    "            )\n",
    "        \n",
    "        for col in volume_cols:\n",
    "            adjustments.append(\n",
    "                pl.when((pl.col('ticker') == ticker) & (pl.col('window_start_date') < execution_date))\n",
    "                .then(pl.col(f'{prefix}{col}') * ratio)\n",
    "                .otherwise(pl.col(f'{prefix}{col}')).alias(f'{prefix}{col}')\n",
    "            )\n",
    "\n",
    "        adjustments.append(\n",
    "            pl.when((pl.col('ticker') == ticker) & (pl.col('window_start_date') < execution_date))\n",
    "            .then(pl.concat_str([pl.col('applied_splits'), pl.lit(f\",{split_str}\")]))\n",
    "            .otherwise(pl.col('applied_splits')).alias('applied_splits')\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(adjustments)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the stock splits to the DataFrame\n",
    "df_split = apply_adjustments(df, splits_df, \"stock\", [\"open\", \"close\", \"high\", \"low\"], [\"volume\"])\n",
    "\n",
    "def format_column(column):\n",
    "    return column.cast(pl.Int64)\n",
    "\n",
    "cols = [\"adjusted_volume\"]\n",
    "\n",
    "# Apply the function to all columns\n",
    "df_split = df_split.with_columns([format_column(pl.col(column)).alias(column) for column in cols])\n",
    "\n",
    "print(\"Stock Splits Applied\")\n",
    "df_split = df_split.sort(\"window_start_est\")\n",
    "df_split.tail(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8edb3-7ea8-48f4-b1a5-2961985bc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######ADD HISTORICAL/REALIZED VOLATILITY\n",
    "\n",
    "def add_historical_volatility(df: pl.DataFrame, period: int = 252) -> pl.DataFrame:\n",
    "    # Ensure the DataFrame is sorted by window_start_est\n",
    "    df = df.sort('window_start_est')\n",
    "\n",
    "    # Extract the date part and calculate daily close prices\n",
    "    daily_df = df.group_by('transaction_date').agg([\n",
    "        pl.col('adjusted_close').last().alias('daily_close')\n",
    "    ])\n",
    "\n",
    "    # Calculate daily returns\n",
    "    daily_df = daily_df.with_columns([\n",
    "        pl.col('daily_close').pct_change().alias('daily_return')\n",
    "    ]).drop_nulls()\n",
    "\n",
    "    # Check if we have enough data for the specified period\n",
    "    if len(daily_df) < period:\n",
    "        raise ValueError(f\"Not enough data for the specified period. Got {len(daily_df)} days, need at least {period}.\")\n",
    "\n",
    "    # Calculate the standard deviation of daily returns for a rolling window\n",
    "    daily_df = daily_df.with_columns([\n",
    "        pl.col('daily_return').rolling_std(window_size=period, min_periods=period)\n",
    "        .mul(252 ** 0.5)  # Annualize\n",
    "        .mul(.01) # divide by 100 for percentage\n",
    "        .alias('hv')\n",
    "    ])\n",
    "\n",
    "    # Join the HV data back to the original DataFrame\n",
    "    df = df.join(daily_df.select(['transaction_date', 'hv']), on='transaction_date', how='left')\n",
    "\n",
    "    # Fill any NaN values in the 'hv' column with the first valid HV value\n",
    "    first_valid_hv = df.select(pl.col('hv').drop_nulls().first()).item()\n",
    "    df = df.with_columns(pl.col('hv').fill_null(first_valid_hv))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_with_hv = add_historical_volatility(df_split)\n",
    "print(\"Historical Volatility calculated\")\n",
    "# df_with_hv.tail(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe67089-b489-4644-a56f-a634a7f1ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******ADD INDICATORS****************\n",
    "import ta\n",
    "\n",
    "df = df_with_hv\n",
    "# Define a function to calculate EMA given a series, span, and adjust flag\n",
    "def ema_custom(series: pl.Series, name: str, span: int) -> pl.Series:\n",
    "\n",
    "    ema = series.ewm_mean(com=span, adjust=False, ignore_nulls=True).alias(name)\n",
    "    return ema\n",
    "\n",
    "df = df.sort('window_start')\n",
    "\n",
    "# Calculate traded value for each transaction\n",
    "df = df.with_columns([\n",
    "    (pl.col('volume') * pl.col('close')).alias('traded_value')\n",
    "])\n",
    "df = df.with_columns([\n",
    "    (pl.col('adjusted_volume') * pl.col('adjusted_close')).alias('adjusted_traded_value')\n",
    "])\n",
    "\n",
    "# Calculate cumulative totals within each trading session\n",
    "df = df.with_columns([\n",
    "    pl.col('traded_value').cum_sum().over(['ticker', 'transaction_date']).alias('daily_traded_value'),\n",
    "    pl.col('volume').cum_sum().over(['ticker', 'transaction_date']).alias('daily_cumulative_volume'),\n",
    "    pl.col('adjusted_traded_value').cum_sum().over(['ticker', 'transaction_date']).alias('adjusted_daily_traded_value'),\n",
    "    pl.col('adjusted_volume').cum_sum().over(['ticker', 'transaction_date']).alias('adjusted_daily_cumulative_volume')\n",
    "])\n",
    "\n",
    "# Calculate the VWAP for each transaction\n",
    "df = df.with_columns([\n",
    "    (pl.col('adjusted_daily_traded_value') / pl.col('adjusted_daily_cumulative_volume')).alias('daily_vwap')\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col('daily_vwap').rolling_mean(window_size=2).over([\"ticker\"]).alias('vwap_2d'),\n",
    "    pl.col('daily_vwap').rolling_mean(window_size=3).over([\"ticker\"]).alias('vwap_3d'),\n",
    "    pl.col('daily_vwap').ewm_mean(com=200, adjust=False, ignore_nulls=True).over([\"ticker\", \"transaction_date\"]).alias('ema_200_vwap')\n",
    "])\n",
    "\n",
    "# Extract hour from 'window_start_est'\n",
    "df = df.with_columns(pl.col('window_start_est').dt.hour().alias('hour'))\n",
    "\n",
    "hourly_vwap_calculation = df.group_by(['ticker', 'transaction_date', 'hour']).agg([\n",
    "    (pl.sum('traded_value').alias('hourly_traded_value')),\n",
    "    (pl.sum('volume').alias('hourly_volume')),\n",
    "    (pl.sum('traded_value') / pl.sum('volume')).alias('hourly_vwap')\n",
    "])\n",
    "\n",
    "# Merge the hourly VWAP back into the original DataFrame\n",
    "df = df.join(\n",
    "    hourly_vwap_calculation,\n",
    "    on=['ticker', 'transaction_date', 'hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df = df.sort(by=['ticker', 'transaction_date'], descending=True)\n",
    "\n",
    "\n",
    "# Define a function to calculate EMA given a series, span, and adjust flag\n",
    "def ema_custom(series: pl.Series, name: str, span: int) -> pl.Series:\n",
    "    alpha = 2 / (span + 1)\n",
    "    ema = series.ewm_mean(com=span, adjust=False, ignore_nulls=True).alias(name)\n",
    "    return ema\n",
    "\n",
    "# Calculate EMA for 12 and 26 periods and add them to the DataFrame\n",
    "df = df.with_columns(\n",
    "    [\n",
    "        ema_custom(df[\"adjusted_close\"], f\"ema_{span}\", span=span)\n",
    "        for span in [12, 26]\n",
    "    ]\n",
    ")\n",
    "\n",
    "def calculate_sma(df, periods=[50, 200]):\n",
    "    for period in periods:\n",
    "        df = df.with_columns(\n",
    "            pl.col('adjusted_close').rolling_mean(window_size=period).alias(f'sma_{period}')\n",
    "        )\n",
    "    return df\n",
    "\n",
    "df = calculate_sma(df, periods=[50, 200])\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "# Calculate the MACD line\n",
    "df = df.with_columns((pl.col(\"ema_12\") - pl.col(\"ema_26\")).alias(\"MACD_line\"))\n",
    "\n",
    "# Calculate the Signal line\n",
    "df = df.with_columns(ema_custom(pl.col(\"MACD_line\"), \"signal_line\", span=9))\n",
    "\n",
    "# Calculate the distance between MACD and Signal lines\n",
    "df = df.with_columns((pl.col(\"MACD_line\") - pl.col(\"signal_line\")).alias(\"MACD_Signal_distance\"))\n",
    "\n",
    "# Calculate the slope of the MACD line (current value - previous value)\n",
    "df = df.with_columns(pl.col(\"MACD_line\").diff().alias(\"MACD_slope\"))\n",
    "\n",
    "# Calculate the slope of the Signal line (current value - previous value)\n",
    "df = df.with_columns(pl.col(\"signal_line\").diff().alias(\"Signal_slope\"))\n",
    "\n",
    "# # Identify MACD line crosses above Signal line (bullish signal)\n",
    "# df = df.with_columns(\n",
    "#     ((pl.col(\"MACD_line\") > pl.col(\"signal_line\")) & \n",
    "#     (pl.col(\"MACD_line\").shift(-1) <= pl.col(\"signal_line\").shift(-1))).cast(pl.Int8).alias(\"MACD_cross_above_Signal\")\n",
    "# )\n",
    "\n",
    "# # Identify MACD line crosses below Signal line (bearish signal)\n",
    "# df = df.with_columns(\n",
    "#     ((pl.col(\"MACD_line\") < pl.col(\"signal_line\")) & \n",
    "#     (pl.col(\"MACD_line\").shift(-1) >= pl.col(\"signal_line\").shift(-1))).cast(pl.Int8).alias(\"MACD_cross_below_Signal\")\n",
    "# )\n",
    "\n",
    "# Identify MACD line crosses above Signal line (bullish signal)\n",
    "df = df.with_columns(\n",
    "    (\n",
    "        (pl.col(\"MACD_line\") > pl.col(\"signal_line\")) &\n",
    "        (pl.col(\"MACD_line\").shift(1) <= pl.col(\"signal_line\").shift(1))\n",
    "    ).cast(pl.Int8).alias(\"MACD_cross_above_Signal\")\n",
    ")\n",
    "\n",
    "# Identify MACD line crosses below Signal line (bearish signal)\n",
    "df = df.with_columns(\n",
    "    (\n",
    "        (pl.col(\"MACD_line\") < pl.col(\"signal_line\")) &\n",
    "        (pl.col(\"MACD_line\").shift(1) >= pl.col(\"signal_line\").shift(1))\n",
    "    ).cast(pl.Int8).alias(\"MACD_cross_below_Signal\")\n",
    ")\n",
    "\n",
    "# Calculate the angle of intersection (in degrees) at the points where the MACD crosses the Signal line\n",
    "# df = df.with_columns([\n",
    "#     pl.when(\n",
    "#         (df[\"MACD_cross_above_Signal\"] == 1) | (df[\"MACD_cross_below_Signal\"] == 1)\n",
    "#     ).then(\n",
    "#         (pl.col(\"MACD_slope\") - pl.col(\"Signal_slope\")).map_elements(np.rad2deg)\n",
    "#     ).otherwise(pl.lit(None)).alias(\"MACD_x_Signal_angle_of_intersection\")\n",
    "# ])\n",
    "\n",
    "# Group by ticker and transaction_date and calculate the sums.\n",
    "daily_crosses = df.group_by(['ticker', 'transaction_date']).agg([\n",
    "    pl.sum('MACD_cross_above_Signal').alias('daily_MACD_crosses_above'),\n",
    "    pl.sum('MACD_cross_below_Signal').alias('daily_MACD_crosses_below')\n",
    "])\n",
    "\n",
    "# Create the flags for crosses above, below, or at zero\n",
    "df = df.with_columns([\n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_cross_above_Signal\") == 1) & (pl.col(\"MACD_line\") < 0)\n",
    "    ).then(True).otherwise(False).alias(\"MACD_x_above_Signal_below_zero\"),\n",
    "    \n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_cross_below_Signal\") == 1) & (pl.col(\"MACD_line\") > 0)\n",
    "    ).then(True).otherwise(False).alias(\"MACD_x_below_Signal_above_zero\"),\n",
    "    \n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_cross_above_Signal\") == 1) & (pl.col(\"MACD_line\") == 0)\n",
    "    ).then(True).otherwise(False).alias(\"MACD_x_above_Signal_at_zero\"),\n",
    "    \n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_cross_below_Signal\") == 1) & (pl.col(\"MACD_line\") == 0)\n",
    "    ).then(True).otherwise(False).alias(\"MACD_x_below_Signal_at_zero\")\n",
    "])\n",
    "\n",
    "df = df.with_columns(\n",
    "    (pl.col('MACD_line') - pl.col('signal_line')).alias('MACD_histogram')\n",
    ")\n",
    "\n",
    "# Add a column for previous histogram values\n",
    "df = df.with_columns(pl.col(\"MACD_histogram\").shift(1).alias(\"previous_histogram\"))\n",
    "\n",
    "# Define the momentum based on the histogram values\n",
    "df = df.with_columns([\n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_histogram\") > 0) & \n",
    "        (pl.col(\"MACD_histogram\") > pl.col(\"previous_histogram\"))\n",
    "    ).then(True)\n",
    "    .otherwise(False)\n",
    "    .alias(\"histogram_bullish_momentum_strengthening\"),\n",
    "\n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_histogram\") < 0) &\n",
    "        (pl.col(\"MACD_histogram\") < pl.col(\"previous_histogram\"))\n",
    "    ).then(True)\n",
    "    .otherwise(False)\n",
    "    .alias(\"histogram_bearish_momentum_strengthening\"),\n",
    "\n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_histogram\") > 0) &\n",
    "        (pl.col(\"MACD_histogram\") < pl.col(\"previous_histogram\"))\n",
    "    ).then(True)\n",
    "    .otherwise(False)\n",
    "    .alias(\"histogram_bullish_momentum_weakening\"),\n",
    "\n",
    "    pl.when(\n",
    "        (pl.col(\"MACD_histogram\") < 0) &\n",
    "        (pl.col(\"MACD_histogram\") > pl.col(\"previous_histogram\"))\n",
    "    ).then(True)\n",
    "    .otherwise(False)\n",
    "    .alias(\"histogram_bearish_momentum_weakening\")\n",
    "])\n",
    "\n",
    "df = df.sort('window_start')\n",
    "\n",
    "prediction_window = 20\n",
    "\n",
    "# Shift the close price by negative prediction window to get future prices\n",
    "df = df.with_columns([\n",
    "    pl.col('close').shift(-prediction_window).alias('20m_future_close')\n",
    "])\n",
    "\n",
    "df = df.with_columns([\n",
    "    ((pl.col('20m_future_close') - pl.col('close')) / pl.col('close')).alias('20m_future_price_change_pct'),\n",
    "    (pl.col('20m_future_close') - pl.col('close')).alias('20m_future_price_change')\n",
    "    # You can add more calculations as needed for your analysis\n",
    "])\n",
    "\n",
    "\n",
    "def calculate_rsi(df, period=14):\n",
    "    # Calculate price change (delta)\n",
    "    df = df.with_columns(\n",
    "        pl.col('close').diff().alias('delta')\n",
    "    )\n",
    "    \n",
    "    # Separate the positive and negative price changes\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('delta') > 0).then(pl.col('delta')).otherwise(0).alias('gain'),\n",
    "        pl.when(pl.col('delta') < 0).then(-pl.col('delta')).otherwise(0).alias('loss')\n",
    "    ])\n",
    "    \n",
    "    # Calculate the rolling average gain and loss\n",
    "    df = df.with_columns([\n",
    "        pl.col('gain').rolling_mean(window_size=period, min_periods=period).alias('avg_gain'),\n",
    "        pl.col('loss').rolling_mean(window_size=period, min_periods=period).alias('avg_loss')\n",
    "    ])\n",
    "    \n",
    "    # Compute the Relative Strength (RS)\n",
    "    df = df.with_columns(\n",
    "        (pl.col('avg_gain') / pl.col('avg_loss')).alias('rs')\n",
    "    )\n",
    "    \n",
    "    # Calculate the RSI\n",
    "    df = df.with_columns(\n",
    "        (100 - (100 / (1 + pl.col('rs')))).alias('rsi')\n",
    "    )\n",
    "    \n",
    "    # Drop intermediate columns if necessary\n",
    "    df = df.drop(['delta', 'gain', 'loss', 'avg_gain', 'avg_loss', 'rs'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = calculate_rsi(df, period=14)\n",
    "\n",
    "\n",
    "def calculate_atr(df, period=14):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            (pl.col('high') - pl.col('low')).alias('hl'),\n",
    "            (pl.col('high') - pl.col('close').shift(1)).abs().alias('hc'),\n",
    "            (pl.col('low') - pl.col('close').shift(1)).abs().alias('lc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Calculate true range as the maximum of the three calculated columns\n",
    "    df = df.with_columns(\n",
    "        pl.max_horizontal(['hl', 'hc', 'lc']).alias('true_range')\n",
    "    )\n",
    "    \n",
    "    # Calculate ATR as the rolling mean of the true range\n",
    "    df = df.with_columns(\n",
    "        pl.col('true_range').rolling_mean(window_size=period).alias('atr')\n",
    "    )\n",
    "    \n",
    "    # Drop intermediate columns if necessary\n",
    "    df = df.drop(['hl', 'hc', 'lc', 'true_range'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df, period=14)\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(df, period=20, std_dev=2):\n",
    "    df = df.with_columns(\n",
    "        pl.col('close').rolling_mean(window_size=period).alias('bb_middle_band')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.col('close').rolling_std(window_size=period).alias('std_dev')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        (pl.col('bb_middle_band') + std_dev * pl.col('std_dev')).alias('bb_upper_band'),\n",
    "        (pl.col('bb_middle_band') - std_dev * pl.col('std_dev')).alias('bb_lower_band')\n",
    "    )\n",
    "    return df.drop(['std_dev'])\n",
    "\n",
    "df = calculate_bollinger_bands(df, period=20, std_dev=2)\n",
    "\n",
    "\n",
    "def calculate_stochastic_oscillator(df, period=14, k=3, d=3):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            (pl.col('close') - pl.col('low').rolling_min(window_size=period)).alias('num'),\n",
    "            (pl.col('high').rolling_max(window_size=period) - pl.col('low').rolling_min(window_size=period)).alias('den')\n",
    "        ]\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        ((pl.col('num') / pl.col('den')) * 100).alias('stochastic_k')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.col('stochastic_k').rolling_mean(window_size=k).alias('stochastic_d')\n",
    "    )\n",
    "    return df.drop(['num', 'den'])\n",
    "\n",
    "df = calculate_stochastic_oscillator(df, period=14, k=3, d=3)\n",
    "\n",
    "\n",
    "def calculate_adx(df, period=14):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            (pl.col('high') - pl.col('high').shift(1)).abs().alias('up_move'),\n",
    "            (pl.col('low') - pl.col('low').shift(1)).abs().alias('down_move')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when((pl.col('up_move') > pl.col('down_move')) & (pl.col('up_move') > 0)).then(pl.col('up_move')).otherwise(0).alias('plus_dm'),\n",
    "            pl.when((pl.col('down_move') > pl.col('up_move')) & (pl.col('down_move') > 0)).then(pl.col('down_move')).otherwise(0).alias('minus_dm')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col('plus_dm').rolling_mean(window_size=period).alias('plus_di'),\n",
    "            pl.col('minus_dm').rolling_mean(window_size=period).alias('minus_di')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        ((pl.col('plus_di') - pl.col('minus_di')).abs() / (pl.col('plus_di') + pl.col('minus_di')) * 100).alias('dx')\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col('dx').rolling_mean(window_size=period).alias('adx')\n",
    "    )\n",
    "\n",
    "    return df.drop(['up_move', 'down_move', 'plus_dm', 'minus_dm', 'plus_di', 'minus_di', 'dx'])\n",
    "\n",
    "df = calculate_adx(df, period=14)\n",
    "\n",
    "#Candlestick Pattern detection\n",
    "def detect_engulfing(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"close\"] > df[\"open\"]) & (df[\"close\"].shift(1) < df[\"open\"].shift(1)) & \n",
    "         (df[\"close\"] > df[\"open\"].shift(1)) & (df[\"open\"] < df[\"close\"].shift(1))).alias(\"bullish_engulfing\"),\n",
    "        ((df[\"close\"] < df[\"open\"]) & (df[\"close\"].shift(1) > df[\"open\"].shift(1)) & \n",
    "         (df[\"close\"] < df[\"open\"].shift(1)) & (df[\"open\"] > df[\"close\"].shift(1))).alias(\"bearish_engulfing\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def detect_momentum(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"high\"] - df[\"low\"]) > 2 * (df[\"high\"].shift(1) - df[\"low\"].shift(1))).alias(\"momentum\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def detect_multiple_wicks(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"high\"] - df[\"close\"]) > (df[\"close\"] - df[\"low\"]) * 2).alias(\"long_upper_wick\"),\n",
    "        ((df[\"close\"] - df[\"low\"]) > (df[\"high\"] - df[\"close\"]) * 2).alias(\"long_lower_wick\")\n",
    "    ])\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        (df[\"long_upper_wick\"].cast(pl.Int32).rolling_sum(window_size=3) >= 2).alias(\"multiple_upper_wicks\"),\n",
    "        (df[\"long_lower_wick\"].cast(pl.Int32).rolling_sum(window_size=3) >= 2).alias(\"multiple_lower_wicks\")\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def detect_doji(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"high\"] - df[\"low\"]) > 0) & (abs(df[\"close\"] - df[\"open\"]) / (df[\"high\"] - df[\"low\"]) < 0.1).alias(\"doji\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "# def detect_hammer(df):\n",
    "#     df = df.with_columns([\n",
    "#         ((df[\"close\"] > df[\"open\"]) & ((df[\"high\"] - df[\"close\"]) <= (df[\"close\"] - df[\"open\"])) & \n",
    "#          ((df[\"close\"] - df[\"open\"]) > 2 * (df[\"open\"] - df[\"low\"]))).alias(\"hammer\")\n",
    "#     ])\n",
    "#     return df\n",
    "\n",
    "\n",
    "def detect_bullish_hammer(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        (\n",
    "            # Lower wick is at least twice the body\n",
    "            (pl.min_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) - pl.col(\"adjusted_low\") >= \n",
    "             2 * (pl.col(\"adjusted_close\") - pl.col(\"adjusted_open\")).abs()) &\n",
    "            \n",
    "            # Upper wick is no more than 10% of lower wick\n",
    "            (pl.col(\"adjusted_high\") - pl.max_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) <= \n",
    "             0.1 * (pl.min_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) - pl.col(\"adjusted_low\"))) &\n",
    "            \n",
    "            # Body is in the upper half of the range\n",
    "            (pl.min_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) >= \n",
    "             (pl.col(\"adjusted_high\") + pl.col(\"adjusted_low\")) / 2) &\n",
    "            \n",
    "            # Close is higher than open\n",
    "            (pl.col(\"adjusted_close\") > pl.col(\"adjusted_open\"))\n",
    "        ).alias(\"bullish_hammer\")\n",
    "    ])\n",
    "\n",
    "def detect_skrong_bullish_hammer(df: pl.DataFrame, min_score: int = 2) -> pl.DataFrame:\n",
    "    return df.with_columns(\n",
    "        (pl.col(\"bullish_hammer_score\") >= min_score).alias(\"skrong_bullish_hammer\")\n",
    "    )\n",
    "\n",
    "def score_bullish_hammer(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        (\n",
    "            pl.when(pl.col(\"bullish_hammer\"))\n",
    "            .then(\n",
    "                (pl.col(\"rsi\") < 30).cast(pl.Int8) +  # Oversold condition\n",
    "                (pl.col(\"adjusted_close\") < pl.col(\"bb_lower_band\")).cast(pl.Int8) +  # Price below lower Bollinger Band\n",
    "                (pl.col(\"MACD_line\") > pl.col(\"signal_line\")).cast(pl.Int8) +  # MACD bullish crossover\n",
    "                (pl.col(\"adx\") > 25).cast(pl.Int8) +  # Strong trend\n",
    "                (pl.col(\"volume\") > pl.col(\"volume\").rolling_mean(window_size=20)).cast(pl.Int8) +  # Above average volume\n",
    "                (pl.col(\"adjusted_close\") < pl.col(\"vwap_2d\")).cast(pl.Int8) +  # Price below 2-day VWAP\n",
    "                (pl.col(\"stochastic_k\") < 20).cast(pl.Int8)  # Stochastic oversold\n",
    "            )\n",
    "            .otherwise(0)\n",
    "        ).alias(\"bullish_hammer_score\")\n",
    "        \n",
    "    ])\n",
    "\n",
    "def detect_bearish_hammer(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        (\n",
    "            # Lower wick is at least twice the body\n",
    "            (pl.min_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) - pl.col(\"adjusted_low\") >= \n",
    "             2 * (pl.col(\"adjusted_close\") - pl.col(\"adjusted_open\")).abs()) &\n",
    "            \n",
    "            # Upper wick is no more than 10% of lower wick\n",
    "            (pl.col(\"adjusted_high\") - pl.max_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) <= \n",
    "             0.1 * (pl.min_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) - pl.col(\"adjusted_low\"))) &\n",
    "            \n",
    "            # Body is in the upper half of the range\n",
    "            (pl.min_horizontal(pl.col(\"adjusted_open\"), pl.col(\"adjusted_close\")) >= \n",
    "             (pl.col(\"adjusted_high\") + pl.col(\"adjusted_low\")) / 2) &\n",
    "            \n",
    "            # Close is lower than open\n",
    "            (pl.col(\"adjusted_close\") < pl.col(\"adjusted_open\"))\n",
    "        ).alias(\"bearish_hammer\")\n",
    "    ])\n",
    "\n",
    "\n",
    "def detect_shooting_star(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"open\"] > df[\"close\"]) & ((df[\"high\"] - df[\"open\"]) <= (df[\"open\"] - df[\"close\"])) & \n",
    "         ((df[\"open\"] - df[\"close\"]) > 2 * (df[\"close\"] - df[\"low\"]))).alias(\"shooting_star\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def detect_tweezer(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"close\"].shift(1) < df[\"open\"].shift(1)) & (df[\"close\"] > df[\"open\"]) & \n",
    "         (df[\"low\"].shift(1) == df[\"low\"])).alias(\"bullish_tweezer\"),\n",
    "        ((df[\"close\"].shift(1) > df[\"open\"].shift(1)) & (df[\"close\"] < df[\"open\"]) & \n",
    "         (df[\"high\"].shift(1) == df[\"high\"])).alias(\"bearish_tweezer\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "def detect_marubozu(df):\n",
    "    df = df.with_columns([\n",
    "        ((df[\"close\"] > df[\"open\"]) & (df[\"low\"] == df[\"open\"]) & (df[\"high\"] == df[\"close\"])).alias(\"bullish_marubozu\"),\n",
    "        ((df[\"close\"] < df[\"open\"]) & (df[\"low\"] == df[\"close\"]) & (df[\"high\"] == df[\"open\"])).alias(\"bearish_marubozu\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_roc(df: pl.DataFrame, period: int) -> pl.DataFrame:\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            (pl.col(\"close\") - pl.col(\"close\").shift(period)).alias(\"price_change\"),\n",
    "            pl.col(\"close\").shift(period).alias(\"price_n_periods_ago\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        ((pl.col(\"price_change\") / pl.col(\"price_n_periods_ago\")) * 100).alias(\"roc\")\n",
    "    )\n",
    "    \n",
    "    # Drop intermediate columns if necessary\n",
    "    df = df.drop(['price_change', 'price_n_periods_ago'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = calculate_roc(df, period=14)\n",
    "\n",
    "# Detecting candlestick patterns\n",
    "df = detect_engulfing(df)\n",
    "df = detect_momentum(df)\n",
    "df = detect_multiple_wicks(df)\n",
    "df = detect_doji(df)\n",
    "df = detect_bullish_hammer(df)\n",
    "df = detect_bearish_hammer(df)\n",
    "df = detect_shooting_star(df)\n",
    "df = detect_tweezer(df)\n",
    "df = detect_marubozu(df)\n",
    "df = score_bullish_hammer(df)\n",
    "df = detect_skrong_bullish_hammer(df)\n",
    "\n",
    "print(\"Indicators Added\")\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def calculate_parabolic_sar(df, step=0.02, max_step=0.2):\n",
    "    logger.info(\"Starting Parabolic SAR calculation\")\n",
    "    \n",
    "    # Store the original schema\n",
    "    original_schema = df.schema\n",
    "    \n",
    "    # Convert datetime columns to integers (nanoseconds since epoch)\n",
    "    datetime_columns = [col for col, dtype in original_schema.items() if isinstance(dtype, pl.Datetime)]\n",
    "    for col in datetime_columns:\n",
    "        df = df.with_columns(pl.col(col).dt.timestamp().cast(pl.Int64).alias(f\"{col}_temp\"))\n",
    "        df = df.drop(col)\n",
    "    \n",
    "    # Initialize columns with default values\n",
    "    df = df.with_columns([\n",
    "        pl.lit(None).cast(pl.Float64).alias('parabolic_sar'),\n",
    "        pl.lit(step).alias('acceleration_factor'),\n",
    "        pl.lit(None).cast(pl.Float64).alias('extreme_point'),\n",
    "        pl.lit(None).cast(pl.Float64).alias('prev_sar')\n",
    "    ])\n",
    "    \n",
    "    sar_data = df.to_dict(as_series=False)\n",
    "    in_uptrend = True\n",
    "    for i in range(1, len(sar_data['close'])):\n",
    "        prior_sar = sar_data['prev_sar'][i-1]\n",
    "        if prior_sar is None:\n",
    "            sar_data['parabolic_sar'][i] = sar_data['low'][i-1]\n",
    "            sar_data['extreme_point'][i] = sar_data['high'][i-1]\n",
    "        else:\n",
    "            if in_uptrend:\n",
    "                sar_data['parabolic_sar'][i] = prior_sar + sar_data['acceleration_factor'][i-1] * (sar_data['extreme_point'][i-1] - prior_sar)\n",
    "                in_uptrend = sar_data['close'][i] > sar_data['parabolic_sar'][i]\n",
    "            else:\n",
    "                sar_data['parabolic_sar'][i] = prior_sar - sar_data['acceleration_factor'][i-1] * (prior_sar - sar_data['extreme_point'][i-1])\n",
    "                in_uptrend = sar_data['close'][i] < sar_data['parabolic_sar'][i]\n",
    "            if in_uptrend:\n",
    "                sar_data['extreme_point'][i] = max(sar_data['extreme_point'][i-1], sar_data['high'][i])\n",
    "            else:\n",
    "                sar_data['extreme_point'][i] = min(sar_data['extreme_point'][i-1], sar_data['low'][i])\n",
    "            if (in_uptrend and sar_data['close'][i] > sar_data['extreme_point'][i-1]) or (not in_uptrend and sar_data['close'][i] < sar_data['extreme_point'][i-1]):\n",
    "                sar_data['acceleration_factor'][i] = min(max_step, sar_data['acceleration_factor'][i-1] + step)\n",
    "            sar_data['prev_sar'][i] = sar_data['parabolic_sar'][i]\n",
    "    \n",
    "    df = pl.DataFrame(sar_data).drop(['acceleration_factor', 'extreme_point', 'prev_sar'])\n",
    "    \n",
    "    # Restore datetime columns\n",
    "    for col in datetime_columns:\n",
    "        df = df.with_columns(pl.from_epoch(pl.col(f\"{col}_temp\"), time_unit=\"ns\").alias(col))\n",
    "        df = df.drop(f\"{col}_temp\")\n",
    "    \n",
    "    # Reapply the original schema\n",
    "    for col, dtype in original_schema.items():\n",
    "        if col in df.columns:\n",
    "            df = df.with_columns(pl.col(col).cast(dtype))\n",
    "    \n",
    "    logger.info(\"Finished calculating Parabolic SAR\")\n",
    "    return df\n",
    "\n",
    "# Fibonacci Retracement (simple example using last significant low and high)\n",
    "def calculate_fibonacci_retracement(df):\n",
    "    logger.info(\"Starting Fibonacci Retracement calculation\")\n",
    "    low = df['low'].min()\n",
    "    high = df['high'].max()\n",
    "    diff = high - low\n",
    "    retracements = [0.236, 0.382, 0.5, 0.618, 0.786]\n",
    "\n",
    "    retracement_levels = {f'fib_{int(level * 100)}': high - diff * level for level in retracements}\n",
    "    \n",
    "    df = df.with_columns([pl.lit(value).alias(key) for key, value in retracement_levels.items()])\n",
    "\n",
    "    logger.info(\"Finished calclating Fibonacci Retracement\")\n",
    "    return df\n",
    "\n",
    "df = calculate_fibonacci_retracement(df)\n",
    "\n",
    "# Ichimoku Cloud\n",
    "def calculate_ichimoku_cloud(df):\n",
    "    logger.info(\"Starting Ichimoku Cloud calculation\")\n",
    "    \n",
    "    # Ensure columns are of type Float64\n",
    "    df = df.with_columns([\n",
    "        pl.col('high').cast(pl.Float64),\n",
    "        pl.col('low').cast(pl.Float64),\n",
    "        pl.col('close').cast(pl.Float64)\n",
    "    ])\n",
    "    \n",
    "    high_9 = df['high'].rolling_max(window_size=9)\n",
    "    low_9 = df['low'].rolling_min(window_size=9)\n",
    "    df = df.with_columns([\n",
    "        ((high_9 + low_9) / 2).alias('tenkan_sen'),\n",
    "        ((df['high'].rolling_max(window_size=26) + df['low'].rolling_min(window_size=26)) / 2).alias('kijun_sen'),\n",
    "    ])\n",
    "    df = df.with_columns(\n",
    "        ((df['tenkan_sen'] + df['kijun_sen']) / 2).shift(26).alias('senkou_span_a')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        ((df['high'].rolling_max(window_size=52) + df['low'].rolling_min(window_size=52)) / 2).shift(26).alias('senkou_span_b')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        df['close'].shift(-26).alias('chikou_span')\n",
    "    )\n",
    "\n",
    "    logger.info(\"Finished Ichimoku Cloud calculation\")\n",
    "    return df\n",
    "\n",
    "df = calculate_ichimoku_cloud(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512c937-0610-423a-b38f-a782c49b37aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter the DataFrame by the date range\n",
    "start_date = \"2024-02-01\"\n",
    "end_date = \"2024-03-30\"\n",
    "\n",
    "# Convert strings to Polars date objects using pl.lit and pl.col\n",
    "filtered_df = df.filter(\n",
    "    (pl.col(\"window_start_est\") >= pl.lit(start_date).cast(pl.Date)) &\n",
    "    (pl.col(\"window_start_est\") <= pl.lit(end_date).cast(pl.Date))\n",
    ")\n",
    "\n",
    "filtered_df.group_by([\"transaction_date\"]).len()\n",
    "# filtered_df.sort(['window_start_est'], descending=True)\n",
    "# len(filtered_df)\n",
    "# df_test.tail(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b03971-a2f5-45f3-b4d1-6b78df098be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.write_csv(\"/Users/brandon/Documents/indicators_240201_240330.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc6d60-55c8-40fc-9b8f-98bfe64a14e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hm = df.filter(pl.col(\"skrong_bullish_hammer\") == True).group_by(['transaction_date']).len().sort('transaction_date')\n",
    "# len(hm)\n",
    "# hm\n",
    "\n",
    "hm = (\n",
    "    df.filter(pl.col(\"skrong_bullish_hammer\") == True)\n",
    "    .groupby(['window_start_est', 'bullish_hammer_score'])\n",
    "    .agg(pl.count())\n",
    "    .sort(['window_start_est', 'bullish_hammer_score'])\n",
    ")\n",
    "len(hm)\n",
    "hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dae9fb-b623-4136-b46e-2a208f36fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE SUPPORT AND RESISTANCE\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "\n",
    "def calculate_pivot_points(df: pl.DataFrame, n1: int, n2: int) -> pl.DataFrame:\n",
    "    def pivotid(l, n1, n2, lows, highs):\n",
    "        if l - n1 < 0 or l + n2 >= len(lows):\n",
    "            return 0\n",
    "        \n",
    "        pividlow = 1\n",
    "        pividhigh = 1\n",
    "        for i in range(l - n1, l + n2 + 1):\n",
    "            if lows[l] > lows[i]:\n",
    "                pividlow = 0\n",
    "            if highs[l] < highs[i]:\n",
    "                pividhigh = 0\n",
    "        \n",
    "        if pividlow and pividhigh:\n",
    "            return 3\n",
    "        elif pividlow:\n",
    "            return 1\n",
    "        elif pividhigh:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    print(f\"STARTING pivot calc at {dt.datetime.now()}\")\n",
    "    lows = df['adjusted_low'].to_numpy()\n",
    "    highs = df['adjusted_high'].to_numpy()\n",
    "    pivot_points = [pivotid(i, n1, n2, lows, highs) for i in range(len(lows))]\n",
    "    \n",
    "    df = df.with_columns(pl.Series(name='pivot', values=pivot_points))\n",
    "    return df\n",
    "\n",
    "def calculate_support_resistance(df: pl.DataFrame, n1: int, n2: int) -> pl.DataFrame:\n",
    "    # Calculate pivot points\n",
    "    df = calculate_pivot_points(df, n1, n2)\n",
    "    \n",
    "    # Initialize columns for support and resistance levels\n",
    "    support_levels = np.full(len(df), np.nan)\n",
    "    resistance_levels = np.full(len(df), np.nan)\n",
    "    \n",
    "    # Identify support and resistance levels\n",
    "    low_series = df['adjusted_low'].to_numpy()\n",
    "    high_series = df['adjusted_high'].to_numpy()\n",
    "    pivot_series = df['pivot'].to_numpy()\n",
    "    \n",
    "    print(f\"STARTING Support and Resistance calc at {dt.datetime.now()}\")\n",
    "    for i in range(len(pivot_series)):\n",
    "        if pivot_series[i] == 1:\n",
    "            support_levels[i] = low_series[i]\n",
    "        elif pivot_series[i] == 2:\n",
    "            resistance_levels[i] = high_series[i]\n",
    "    \n",
    "    # Add the support and resistance levels back to the DataFrame\n",
    "    df = df.with_columns([\n",
    "        pl.Series(name='support_level', values=support_levels),\n",
    "        pl.Series(name='resistance_level', values=resistance_levels)\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_weighted_support_resistance(df: pl.DataFrame, threshold: float = 0.01) -> pl.DataFrame:\n",
    "    # Convert Polars DataFrame to NumPy arrays for faster calculations\n",
    "    support_levels = df['support_level'].drop_nulls().unique().to_numpy()\n",
    "    resistance_levels = df['resistance_level'].drop_nulls().unique().to_numpy()\n",
    "    low_prices = df['adjusted_low'].to_numpy()\n",
    "    high_prices = df['adjusted_high'].to_numpy()\n",
    "\n",
    "    # Initialize dictionaries to count touches\n",
    "    support_touch_counts = {}\n",
    "    resistance_touch_counts = {}\n",
    "\n",
    "    # Count touches for support levels\n",
    "    for level in support_levels:\n",
    "        support_touch_counts[level] = np.sum((low_prices >= (level * (1 - threshold))) &\n",
    "                                             (low_prices <= (level * (1 + threshold))))\n",
    "\n",
    "    # Count touches for resistance levels\n",
    "    for level in resistance_levels:\n",
    "        resistance_touch_counts[level] = np.sum((high_prices >= (level * (1 - threshold))) & \n",
    "                                                (high_prices <= (level * (1 + threshold))))\n",
    "\n",
    "    # Group and normalize levels\n",
    "    significant_support_levels = {level: count for level, count in support_touch_counts.items() if count >= 3}\n",
    "    significant_resistance_levels = {level: count for level, count in resistance_touch_counts.items() if count >= 3}\n",
    "\n",
    "    def group_levels(levels):\n",
    "        grouped_levels = {}\n",
    "        for level, count in levels.items():\n",
    "            found = False\n",
    "            for key in grouped_levels:\n",
    "                if abs(level - key) <= threshold * key:\n",
    "                    grouped_levels[key] += count\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                grouped_levels[level] = count\n",
    "        return grouped_levels\n",
    "\n",
    "    grouped_support_levels = group_levels(significant_support_levels)\n",
    "    grouped_resistance_levels = group_levels(significant_resistance_levels)\n",
    "\n",
    "    # Add significant levels back to the DataFrame\n",
    "    df = df.with_columns([\n",
    "        pl.col('support_level').map_elements(lambda x: x if x in grouped_support_levels else np.nan).alias('weighted_support_level'),\n",
    "        pl.col('resistance_level').map_elements(lambda x: x if x in grouped_resistance_levels else np.nan).alias('weighted_resistance_level')\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "    \n",
    "def preprocess_with_support_resistance(df: pl.DataFrame, n1: int = 10, n2: int = 10) -> pl.DataFrame:\n",
    "    try:\n",
    "        df = calculate_support_resistance(df, n1, n2)\n",
    "        df = calculate_weighted_support_resistance(df)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        current_time = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        raise RuntimeError(f\"Error at {current_time}: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    print(f\"STARTING SRChannel calc at {dt.datetime.now()}\")\n",
    "    df = preprocess_with_support_resistance(df)\n",
    "    print(f\"FINISHED SRChannel calc at {dt.datetime.now()}\")\n",
    "    # df.tail(1000)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5819e99-5e8d-4cea-ad07-fd105b3b4dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"symbol\": \"NVDA\",\n",
      "  \"date\": \"2024-02-23\",\n",
      "  \"underlying_stock_price\": 78.85,\n",
      "  \"option_type\": \"Call\",\n",
      "  \"strike_price\": 80,\n",
      "  \"entry_price\": 79.5,\n",
      "  \"exit_price\": 80.5,\n",
      "  \"recommendation\": \"Buy\",\n",
      "  \"confidence_level\": 75,\n",
      "  \"support_levels\": [78.5, 78],\n",
      "  \"resistance_levels\": [80, 81],\n",
      "  \"indicators_summary\": \"The stock is showing signs of potential upward movement with the price approaching resistance levels. The RSI is neutral around 50-60, and MACD shows convergence. Volume is relatively high, suggesting strong market participation.\",\n",
      "  \"additional_notes\": \"Monitor closely for any divergence in RSI or MACD, and ensure to evaluate the market context. Set a stop-loss at 77.5 to mitigate potential downside risk.\"\n",
      "}\n",
      "```\n",
      "Recommendations generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = \"sk-proj-BTvkAwAGbdgQF19UGfXFT3BlbkFJMEsTpMLsh9RpHkiEvX3m\"\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Base directory where the files are stored\n",
    "base_dir = os.path.expanduser(\"/Users/brandon/Documents/chart_images/20240223145500\")\n",
    "\n",
    "# Your res_format definition (as provided in your code)\n",
    "res_format = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"symbol\": {\"type\": \"string\", \"description\": \"The ticker symbol.\"},\n",
    "        \"date\": {\"type\": \"string\", \"format\": \"date\", \"description\": \"Date in YYYY-MM-DD.\"},\n",
    "        \"underlying_stock_price\": {\"type\": \"number\", \"description\": \"Stock price.\"},\n",
    "        \"option_type\": {\"type\": \"string\", \"enum\": [\"Call\", \"Put\"], \"description\": \"Option type.\"},\n",
    "        \"strike_price\": {\"type\": \"number\", \"description\": \"Strike price.\"},\n",
    "        \"entry_price\": {\"type\": \"number\", \"description\": \"Entry price.\"},\n",
    "        \"exit_price\": {\"type\": \"number\", \"description\": \"Exit price.\"},\n",
    "        \"recommendation\": {\"type\": \"string\", \"enum\": [\"Buy\", \"Sell\", \"Hold\"], \"description\": \"Recommendation.\"},\n",
    "        \"confidence_level\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100, \"description\": \"Confidence level in %.\"},\n",
    "        \"support_levels\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Support levels.\"},\n",
    "        \"resistance_levels\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Resistance levels.\"},\n",
    "        \"indicators_summary\": {\"type\": \"string\", \"description\": \"Summary of key indicators.\"},\n",
    "        \"additional_notes\": {\"type\": \"string\", \"description\": \"Additional notes.\"}\n",
    "    },\n",
    "    \"required\": [\"symbol\", \"date\", \"option_type\", \"strike_price\", \"recommendation\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# Function to extract key metrics from the CSV\n",
    "def extract_metrics(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Extract or calculate key metrics from the CSV\n",
    "    metrics = df.describe().to_string()\n",
    "    return metrics\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def generate_recommendation(indicators, image_path):\n",
    "    # Encode the image\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"You are a financial analyst. Analyze the chart and provide a recommendation\n",
    "                            based on the chart and the following indicators. \n",
    "                            Use the following JSON format: {json.dumps(res_format, indent=2)} \n",
    "                            as a template to fill in for your response with the answers. Please only return the JSON\n",
    "                            reponse, so I can consistently parse your answers. Only respond with a parsable json object, \n",
    "                            and please omit the schema since I already have it.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Here are the key indicators:\\n{indicators}\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "directories = [\"/Users/brandon/Documents/chart_images/20240223145500\"]\n",
    "\n",
    "# Run the analysis for each directory\n",
    "for directory in directories:\n",
    "    # Extract indicators from CSV\n",
    "    indicators = extract_metrics(os.path.join(directory, \"indicators_20240223145500.csv\"))\n",
    "    \n",
    "    # Generate the recommendation\n",
    "    image_path = os.path.join(directory, \"daily_chart_20240223145500.png\")\n",
    "    recommendation = generate_recommendation(indicators, image_path)\n",
    "    print(recommendation)\n",
    "    \n",
    "    # Save the recommendation\n",
    "    with open(os.path.join(directory, \"recommendation.txt\"), \"w\") as f:\n",
    "        f.write(recommendation)\n",
    "\n",
    "print(\"Recommendations generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6ab18e-2c0f-4b2f-a5eb-e9d9a94444dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SyncPage[Model]' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m models \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mlist()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Print the names of the models you have access to\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SyncPage[Model]' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print the names of the models you have access to\n",
    "for model in models['data']:\n",
    "    print(model['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a3f848-5496-4b96-88fd-ea9f67962ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared and recreated directory: /Volumes/WD18TB/chart_images\n",
      "Processing: 2024-02-01 09:30:00-05:00\n",
      "Processing: 2024-02-01 09:31:00-05:00\n",
      "Processing: 2024-02-01 09:32:00-05:00\n",
      "Processing: 2024-02-01 09:33:00-05:00\n",
      "Processing: 2024-02-01 09:34:00-05:00\n",
      "Processing: 2024-02-01 09:35:00-05:00\n",
      "Processing: 2024-02-01 09:36:00-05:00\n",
      "Processing: 2024-02-01 09:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:37:00-05:00.png\n",
      "Processing: 2024-02-01 09:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:32:00-05:00.png\n",
      "Processing: 2024-02-01 09:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:33:00-05:00.png\n",
      "Processing: 2024-02-01 09:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:36:00-05:00.png\n",
      "Processing: 2024-02-01 09:41:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:34:00-05:00.png\n",
      "Processing: 2024-02-01 09:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:31:00-05:00.png\n",
      "Processing: 2024-02-01 09:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:35:00-05:00.png\n",
      "Processing: 2024-02-01 09:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:30:00-05:00.png\n",
      "Processing: 2024-02-01 09:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:38:00-05:00.png\n",
      "Processing: 2024-02-01 09:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:39:00-05:00.png\n",
      "Processing: 2024-02-01 09:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:40:00-05:00.png\n",
      "Processing: 2024-02-01 09:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:41:00-05:00.png\n",
      "Processing: 2024-02-01 09:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:42:00-05:00.png\n",
      "Processing: 2024-02-01 09:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:43:00-05:00.png\n",
      "Processing: 2024-02-01 09:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:44:00-05:00.png\n",
      "Processing: 2024-02-01 09:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:45:00-05:00.png\n",
      "Processing: 2024-02-01 09:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:46:00-05:00.png\n",
      "Processing: 2024-02-01 09:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:47:00-05:00.png\n",
      "Processing: 2024-02-01 09:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:48:00-05:00.png\n",
      "Processing: 2024-02-01 09:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:49:00-05:00.png\n",
      "Processing: 2024-02-01 09:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:50:00-05:00.png\n",
      "Processing: 2024-02-01 09:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:51:00-05:00.png\n",
      "Processing: 2024-02-01 09:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:52:00-05:00.png\n",
      "Processing: 2024-02-01 10:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:53:00-05:00.png\n",
      "Processing: 2024-02-01 10:01:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:54:00-05:00.png\n",
      "Processing: 2024-02-01 10:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:55:00-05:00.png\n",
      "Processing: 2024-02-01 10:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:56:00-05:00.png\n",
      "Processing: 2024-02-01 10:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:57:00-05:00.png\n",
      "Processing: 2024-02-01 10:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:58:00-05:00.png\n",
      "Processing: 2024-02-01 10:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 09:59:00-05:00.png\n",
      "Processing: 2024-02-01 10:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:00:00-05:00.png\n",
      "Processing: 2024-02-01 10:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:01:00-05:00.png\n",
      "Processing: 2024-02-01 10:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:02:00-05:00.png\n",
      "Processing: 2024-02-01 10:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:03:00-05:00.png\n",
      "Processing: 2024-02-01 10:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:04:00-05:00.png\n",
      "Processing: 2024-02-01 10:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:05:00-05:00.png\n",
      "Processing: 2024-02-01 10:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:06:00-05:00.png\n",
      "Processing: 2024-02-01 10:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:07:00-05:00.png\n",
      "Processing: 2024-02-01 10:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:08:00-05:00.png\n",
      "Processing: 2024-02-01 10:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:09:00-05:00.png\n",
      "Processing: 2024-02-01 10:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:10:00-05:00.png\n",
      "Processing: 2024-02-01 10:18:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:11:00-05:00.png\n",
      "Processing: 2024-02-01 10:19:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:12:00-05:00.png\n",
      "Processing: 2024-02-01 10:20:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:13:00-05:00.png\n",
      "Processing: 2024-02-01 10:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:14:00-05:00.png\n",
      "Processing: 2024-02-01 10:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:15:00-05:00.png\n",
      "Processing: 2024-02-01 10:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:16:00-05:00.png\n",
      "Processing: 2024-02-01 10:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:17:00-05:00.png\n",
      "Processing: 2024-02-01 10:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:18:00-05:00.png\n",
      "Processing: 2024-02-01 10:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:19:00-05:00.png\n",
      "Processing: 2024-02-01 10:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:20:00-05:00.png\n",
      "Processing: 2024-02-01 10:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:21:00-05:00.png\n",
      "Processing: 2024-02-01 10:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:22:00-05:00.png\n",
      "Processing: 2024-02-01 10:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:23:00-05:00.png\n",
      "Processing: 2024-02-01 10:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:24:00-05:00.png\n",
      "Processing: 2024-02-01 10:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:25:00-05:00.png\n",
      "Processing: 2024-02-01 10:33:00-05:00\n",
      "Processing: 2024-02-01 10:34:00-05:00\n",
      "Processing: 2024-02-01 10:35:00-05:00\n",
      "Processing: 2024-02-01 10:36:00-05:00\n",
      "Processing: 2024-02-01 10:37:00-05:00\n",
      "Processing: 2024-02-01 10:38:00-05:00\n",
      "Processing: 2024-02-01 10:39:00-05:00\n",
      "Processing: 2024-02-01 10:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:35:00-05:00.png\n",
      "Processing: 2024-02-01 10:41:00-05:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 257\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Example usage with configurable workers\u001b[39;00m\n\u001b[1;32m    256\u001b[0m max_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m--> 257\u001b[0m \u001b[43mexecute_concurrently\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_timestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 252\u001b[0m, in \u001b[0;36mexecute_concurrently\u001b[0;34m(unique_timestamps, max_workers, clear_dir)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleared and recreated directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Execute the function concurrently for each timestamp\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_timestamps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:34:00-05:00.png\n",
      "Processing: 2024-02-01 10:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:39:00-05:00.png\n",
      "Processing: 2024-02-01 10:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:33:00-05:00.png\n",
      "Processing: 2024-02-01 10:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:37:00-05:00.png\n",
      "Processing: 2024-02-01 10:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:38:00-05:00.png\n",
      "Processing: 2024-02-01 10:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:40:00-05:00.png\n",
      "Processing: 2024-02-01 10:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:36:00-05:00.png\n",
      "Processing: 2024-02-01 10:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:41:00-05:00.png\n",
      "Processing: 2024-02-01 10:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:42:00-05:00.png\n",
      "Processing: 2024-02-01 10:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:43:00-05:00.png\n",
      "Processing: 2024-02-01 10:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:44:00-05:00.png\n",
      "Processing: 2024-02-01 10:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:45:00-05:00.png\n",
      "Processing: 2024-02-01 10:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:46:00-05:00.png\n",
      "Processing: 2024-02-01 10:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:47:00-05:00.png\n",
      "Processing: 2024-02-01 10:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:48:00-05:00.png\n",
      "Processing: 2024-02-01 10:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:49:00-05:00.png\n",
      "Processing: 2024-02-01 10:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:50:00-05:00.png\n",
      "Processing: 2024-02-01 10:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:51:00-05:00.png\n",
      "Processing: 2024-02-01 10:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:52:00-05:00.png\n",
      "Processing: 2024-02-01 11:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:53:00-05:00.png\n",
      "Processing: 2024-02-01 11:01:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:54:00-05:00.png\n",
      "Processing: 2024-02-01 11:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:55:00-05:00.png\n",
      "Processing: 2024-02-01 11:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:56:00-05:00.png\n",
      "Processing: 2024-02-01 11:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:57:00-05:00.png\n",
      "Processing: 2024-02-01 11:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:58:00-05:00.png\n",
      "Processing: 2024-02-01 11:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 10:59:00-05:00.png\n",
      "Processing: 2024-02-01 11:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:00:00-05:00.png\n",
      "Processing: 2024-02-01 11:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:01:00-05:00.png\n",
      "Processing: 2024-02-01 11:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:02:00-05:00.png\n",
      "Processing: 2024-02-01 11:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:03:00-05:00.png\n",
      "Processing: 2024-02-01 11:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:04:00-05:00.png\n",
      "Processing: 2024-02-01 11:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:05:00-05:00.png\n",
      "Processing: 2024-02-01 11:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:06:00-05:00.png\n",
      "Processing: 2024-02-01 11:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:07:00-05:00.png\n",
      "Processing: 2024-02-01 11:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:08:00-05:00.png\n",
      "Processing: 2024-02-01 11:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:09:00-05:00.png\n",
      "Processing: 2024-02-01 11:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:10:00-05:00.png\n",
      "Processing: 2024-02-01 11:18:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:11:00-05:00.png\n",
      "Processing: 2024-02-01 11:19:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:12:00-05:00.png\n",
      "Processing: 2024-02-01 11:20:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:13:00-05:00.png\n",
      "Processing: 2024-02-01 11:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:14:00-05:00.png\n",
      "Processing: 2024-02-01 11:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:15:00-05:00.png\n",
      "Processing: 2024-02-01 11:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:16:00-05:00.png\n",
      "Processing: 2024-02-01 11:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:17:00-05:00.png\n",
      "Processing: 2024-02-01 11:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:18:00-05:00.png\n",
      "Processing: 2024-02-01 11:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:19:00-05:00.png\n",
      "Processing: 2024-02-01 11:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:20:00-05:00.png\n",
      "Processing: 2024-02-01 11:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:21:00-05:00.png\n",
      "Processing: 2024-02-01 11:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:22:00-05:00.png\n",
      "Processing: 2024-02-01 11:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:23:00-05:00.png\n",
      "Processing: 2024-02-01 11:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:24:00-05:00.png\n",
      "Processing: 2024-02-01 11:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:25:00-05:00.png\n",
      "Processing: 2024-02-01 11:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:26:00-05:00.png\n",
      "Processing: 2024-02-01 11:34:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:27:00-05:00.png\n",
      "Processing: 2024-02-01 11:35:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:28:00-05:00.png\n",
      "Processing: 2024-02-01 11:36:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:29:00-05:00.png\n",
      "Processing: 2024-02-01 11:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:30:00-05:00.png\n",
      "Processing: 2024-02-01 11:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:31:00-05:00.png\n",
      "Processing: 2024-02-01 11:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:32:00-05:00.png\n",
      "Processing: 2024-02-01 11:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:33:00-05:00.png\n",
      "Processing: 2024-02-01 11:41:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:34:00-05:00.png\n",
      "Processing: 2024-02-01 11:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:35:00-05:00.png\n",
      "Processing: 2024-02-01 11:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:36:00-05:00.png\n",
      "Processing: 2024-02-01 11:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:37:00-05:00.png\n",
      "Processing: 2024-02-01 11:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:38:00-05:00.png\n",
      "Processing: 2024-02-01 11:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:39:00-05:00.png\n",
      "Processing: 2024-02-01 11:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:40:00-05:00.png\n",
      "Processing: 2024-02-01 11:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:41:00-05:00.png\n",
      "Processing: 2024-02-01 11:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:42:00-05:00.png\n",
      "Processing: 2024-02-01 11:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:43:00-05:00.png\n",
      "Processing: 2024-02-01 11:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:44:00-05:00.png\n",
      "Processing: 2024-02-01 11:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:45:00-05:00.png\n",
      "Processing: 2024-02-01 11:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:46:00-05:00.png\n",
      "Processing: 2024-02-01 11:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:47:00-05:00.png\n",
      "Processing: 2024-02-01 11:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:48:00-05:00.png\n",
      "Processing: 2024-02-01 11:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:49:00-05:00.png\n",
      "Processing: 2024-02-01 11:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:50:00-05:00.pngProcessing: 2024-02-01 11:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:51:00-05:00.png\n",
      "Processing: 2024-02-01 11:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:52:00-05:00.png\n",
      "Processing: 2024-02-01 12:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:53:00-05:00.png\n",
      "Processing: 2024-02-01 12:01:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:54:00-05:00.png\n",
      "Processing: 2024-02-01 12:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:55:00-05:00.png\n",
      "Processing: 2024-02-01 12:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:56:00-05:00.png\n",
      "Processing: 2024-02-01 12:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:57:00-05:00.png\n",
      "Processing: 2024-02-01 12:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:58:00-05:00.png\n",
      "Processing: 2024-02-01 12:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 11:59:00-05:00.png\n",
      "Processing: 2024-02-01 12:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:00:00-05:00.png\n",
      "Processing: 2024-02-01 12:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:01:00-05:00.png\n",
      "Processing: 2024-02-01 12:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:02:00-05:00.png\n",
      "Processing: 2024-02-01 12:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:03:00-05:00.png\n",
      "Processing: 2024-02-01 12:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:04:00-05:00.png\n",
      "Processing: 2024-02-01 12:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:05:00-05:00.png\n",
      "Processing: 2024-02-01 12:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:06:00-05:00.png\n",
      "Processing: 2024-02-01 12:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:07:00-05:00.png\n",
      "Processing: 2024-02-01 12:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:08:00-05:00.png\n",
      "Processing: 2024-02-01 12:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:09:00-05:00.png\n",
      "Processing: 2024-02-01 12:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:10:00-05:00.png\n",
      "Processing: 2024-02-01 12:18:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:11:00-05:00.png\n",
      "Processing: 2024-02-01 12:19:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:12:00-05:00.png\n",
      "Processing: 2024-02-01 12:20:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:13:00-05:00.png\n",
      "Processing: 2024-02-01 12:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:14:00-05:00.png\n",
      "Processing: 2024-02-01 12:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:15:00-05:00.png\n",
      "Processing: 2024-02-01 12:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:16:00-05:00.png\n",
      "Processing: 2024-02-01 12:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:17:00-05:00.png\n",
      "Processing: 2024-02-01 12:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:18:00-05:00.png\n",
      "Processing: 2024-02-01 12:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:19:00-05:00.png\n",
      "Processing: 2024-02-01 12:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:20:00-05:00.png\n",
      "Processing: 2024-02-01 12:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:21:00-05:00.png\n",
      "Processing: 2024-02-01 12:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:22:00-05:00.png\n",
      "Processing: 2024-02-01 12:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:23:00-05:00.png\n",
      "Processing: 2024-02-01 12:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:24:00-05:00.png\n",
      "Processing: 2024-02-01 12:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:25:00-05:00.png\n",
      "Processing: 2024-02-01 12:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:26:00-05:00.png\n",
      "Processing: 2024-02-01 12:34:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:27:00-05:00.png\n",
      "Processing: 2024-02-01 12:35:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:28:00-05:00.png\n",
      "Processing: 2024-02-01 12:36:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:29:00-05:00.png\n",
      "Processing: 2024-02-01 12:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:30:00-05:00.png\n",
      "Processing: 2024-02-01 12:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:31:00-05:00.png\n",
      "Processing: 2024-02-01 12:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:32:00-05:00.png\n",
      "Processing: 2024-02-01 12:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:33:00-05:00.png\n",
      "Processing: 2024-02-01 12:41:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:34:00-05:00.png\n",
      "Processing: 2024-02-01 12:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:35:00-05:00.png\n",
      "Processing: 2024-02-01 12:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:36:00-05:00.png\n",
      "Processing: 2024-02-01 12:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:37:00-05:00.png\n",
      "Processing: 2024-02-01 12:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:38:00-05:00.png\n",
      "Processing: 2024-02-01 12:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:39:00-05:00.png\n",
      "Processing: 2024-02-01 12:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:40:00-05:00.png\n",
      "Processing: 2024-02-01 12:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:41:00-05:00.png\n",
      "Processing: 2024-02-01 12:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:42:00-05:00.png\n",
      "Processing: 2024-02-01 12:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:43:00-05:00.png\n",
      "Processing: 2024-02-01 12:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:44:00-05:00.png\n",
      "Processing: 2024-02-01 12:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:45:00-05:00.png\n",
      "Processing: 2024-02-01 12:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:46:00-05:00.png\n",
      "Processing: 2024-02-01 12:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:47:00-05:00.png\n",
      "Processing: 2024-02-01 12:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:48:00-05:00.png\n",
      "Processing: 2024-02-01 12:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:49:00-05:00.png\n",
      "Processing: 2024-02-01 12:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:50:00-05:00.png\n",
      "Processing: 2024-02-01 12:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:51:00-05:00.png\n",
      "Processing: 2024-02-01 12:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:52:00-05:00.png\n",
      "Processing: 2024-02-01 13:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:53:00-05:00.png\n",
      "Processing: 2024-02-01 13:01:00-05:00\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import polars as pl\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, time, timedelta\n",
    "\n",
    "def generate_and_save_chart(df, ticker, window_start, window_end, filename):\n",
    "    # Filter the Polars DataFrame to only include data within the specified window\n",
    "    filtered_df = df.filter(\n",
    "        (pl.col('ticker') == ticker) & \n",
    "        (pl.col('window_start_est') >= pl.lit(window_start)) & \n",
    "        (pl.col('window_start_est') <= pl.lit(window_end))\n",
    "    )\n",
    "\n",
    "    # Determine crossing points\n",
    "    cross_above = filtered_df.filter(pl.col('MACD_cross_above_Signal') == 1)['window_start_est']\n",
    "    cross_below = filtered_df.filter(pl.col('MACD_cross_below_Signal') == 1)['window_start_est']\n",
    "\n",
    "    # Convert to pandas for Plotly compatibility\n",
    "    filtered_df_pd = filtered_df.to_pandas()\n",
    "    \n",
    "    # Create a subplot with 4 rows\n",
    "    # fig = make_subplots(\n",
    "    #     rows=4, cols=1, shared_xaxes=True, vertical_spacing=0.1,  # Increased spacing\n",
    "    #     row_heights=[0.5, 0.2, 0.15, 0.15],  # Adjust the heights of the subplots\n",
    "    #     subplot_titles=(\n",
    "    #         f'{ticker} Stock Price with 2-day and 3-day VWAP',\n",
    "    #         'MACD & Signal Line',\n",
    "    #         'adjusted_volume',\n",
    "    #         'RSI'\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    fig = make_subplots(\n",
    "    rows=4, cols=1, \n",
    "    shared_xaxes=True, \n",
    "    vertical_spacing=0.1,\n",
    "    row_heights=[0.5, 0.2, 0.15, 0.15],\n",
    "    subplot_titles=(\n",
    "        f'{ticker} Stock Price with VWAPs',\n",
    "        'MACD & Signal',\n",
    "        'Volume',\n",
    "        'RSI'\n",
    "    ),\n",
    "    figure=go.Figure(layout=go.Layout(height=800, width=1000))  # Increase figure size\n",
    ")\n",
    "\n",
    "    # Add the candlestick chart to the first row\n",
    "    fig.add_trace(\n",
    "        go.Candlestick(\n",
    "            x=filtered_df_pd['window_start_est'], open=filtered_df_pd['adjusted_open'],\n",
    "            high=filtered_df_pd['adjusted_high'], low=filtered_df_pd['adjusted_low'],\n",
    "            close=filtered_df_pd['adjusted_close']\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Adding the 2-day VWAP to the candlestick chart\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_df_pd['window_start_est'], y=filtered_df_pd['vwap_2d'],\n",
    "            mode='lines', name='2-day VWAP', line=dict(width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Adding the 3-day VWAP to the candlestick chart\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_df_pd['window_start_est'], y=filtered_df_pd['vwap_3d'],\n",
    "            mode='lines', name='3-day VWAP', line=dict(width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Adding the ema_200 to the candlestick chart\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_df_pd['window_start_est'], y=filtered_df_pd['ema_200_vwap'],\n",
    "            mode='lines', name='EMA 200', line=dict(width=2)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add support and resistance lines to the candlestick chart\n",
    "    support_levels = filtered_df_pd['support_level'].dropna().unique()\n",
    "    resistance_levels = filtered_df_pd['resistance_level'].dropna().unique()\n",
    "\n",
    "    for level in support_levels:\n",
    "        fig.add_hline(y=level, line=dict(color=\"green\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "    for level in resistance_levels:\n",
    "        fig.add_hline(y=level, line=dict(color=\"red\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "    # Add MACD line to the second row\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_df_pd['window_start_est'], y=filtered_df_pd['MACD_line'],\n",
    "            mode='lines', name='MACD Line'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Add Signal line to the second row\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_df_pd['window_start_est'], y=filtered_df_pd['signal_line'],\n",
    "            mode='lines', name='Signal Line'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Add vertical lines for MACD crossing above Signal line\n",
    "    for cross_time in cross_above:\n",
    "        fig.add_vline(\n",
    "            x=cross_time, line=dict(color=\"green\", width=2),\n",
    "            row='all', col=1\n",
    "        )\n",
    "\n",
    "    # Add vertical lines for MACD crossing below Signal line\n",
    "    for cross_time in cross_below:\n",
    "        fig.add_vline(\n",
    "            x=cross_time, line=dict(color=\"red\", width=2),\n",
    "            row='all', col=1\n",
    "        )\n",
    "\n",
    "    # Add MACD histogram to the second row (below MACD and Signal Line)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=filtered_df_pd['window_start_est'],\n",
    "            y=filtered_df_pd['MACD_histogram'],\n",
    "            name='MACD Histogram',\n",
    "            marker_color=np.where(filtered_df_pd['MACD_histogram'] >= 0, 'green', 'red')  # color bars conditionally\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Add horizontal line at y=0 in the MACD plot\n",
    "    fig.add_hline(y=0, line=dict(color=\"black\", width=2, dash=\"solid\"), row=2, col=1)\n",
    "\n",
    "    # Add volume bars to the third row\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=filtered_df_pd['window_start_est'],\n",
    "            y=filtered_df_pd['adjusted_volume'],\n",
    "            marker_color='blue',\n",
    "            name='adjusted_volume'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "    # Add RSI line to the fourth row\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=filtered_df_pd['window_start_est'], y=filtered_df_pd['rsi'],\n",
    "            mode='lines', name='RSI'\n",
    "        ),\n",
    "        row=4, col=1\n",
    "    )\n",
    "\n",
    "    # Add horizontal lines at RSI levels 30 and 70\n",
    "    fig.add_hline(y=30, line=dict(color=\"red\", width=1, dash=\"dash\"), row=4, col=1)\n",
    "    fig.add_hline(y=70, line=dict(color=\"red\", width=1, dash=\"dash\"), row=4, col=1)\n",
    "\n",
    "    # Updating layout for better readability\n",
    "    # fig.update_layout(\n",
    "    #     title=f'{ticker} Stock Price and Technical Indicators on {window_end}',\n",
    "    #     xaxis_title='Time',\n",
    "    #     yaxis_title='Price (USD)',\n",
    "    #     legend_title_text='Indicator',\n",
    "    #     xaxis_rangeslider_visible=False\n",
    "    # )\n",
    "\n",
    "    # # Adjust x-axis and y-axis titles for subplots\n",
    "    # fig.update_xaxes(title_text=\"Time\", row=1, col=1, tickformat=\"%H:%M\")\n",
    "    # fig.update_xaxes(title_text=\"Time\", row=2, col=1, tickformat=\"%H:%M\")\n",
    "    # fig.update_xaxes(title_text=\"Time\", row=3, col=1, tickformat=\"%H:%M\")\n",
    "    # fig.update_xaxes(title_text=\"Time\", row=4, col=1, tickformat=\"%H:%M\")\n",
    "\n",
    "    # fig.update_yaxes(title_text=\"Price (USD)\", row=1, col=1)\n",
    "    # fig.update_yaxes(title_text=\"MACD\", row=2, col=1)\n",
    "    # fig.update_yaxes(title_text='adjusted_volume', row=3, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'{ticker} Stock Price and Technical Indicators on {window_end.strftime(\"%Y-%m-%d %H:%M %Z\")}',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Price (USD)',\n",
    "        legend_title_text='Indicator',\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        font=dict(size=10)  # Reduce overall font size\n",
    "    )\n",
    "\n",
    "    # Update x-axes\n",
    "    for i in range(1, 5):\n",
    "        fig.update_xaxes(\n",
    "            title_text=\"Time\" if i == 4 else None,  # Only show \"Time\" on bottom subplot\n",
    "            row=i, col=1,\n",
    "            tickangle=45,  # Rotate labels\n",
    "            nticks=6  # Reduce number of ticks\n",
    "        )\n",
    "\n",
    "    # Update y-axes\n",
    "    fig.update_yaxes(title_text=\"Price (USD)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"MACD\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Vol', row=3, col=1)\n",
    "    fig.update_yaxes(title_text='RSI', row=4, col=1)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    fig.write_image(filename)\n",
    "\n",
    "# Loop through each minute bar in the dataset and generate the chart up to that minute\n",
    "trading_start_time = time(9, 30)\n",
    "trading_end_time = time(16, 0)\n",
    "\n",
    "# Convert the window start and end to the correct timezone\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "\n",
    "\n",
    "# print(unique_timestamps)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "unique_timestamps = filtered_df.select(pl.col('window_start_est')).unique().sort('window_start_est').to_series().to_list()\n",
    "unique_timestamps = [\n",
    "    ts for ts in unique_timestamps\n",
    "    if ts.time() >= time(9, 30) and ts.time() <= time(16, 0)\n",
    "]\n",
    "\n",
    "def process_timestamp(timestamp):\n",
    "    print(f\"Processing: {timestamp}\")\n",
    "    # Ensure window_start is set to 9:30 AM Eastern time for the date of the timestamp\n",
    "    window_start_date = timestamp.date()\n",
    "    window_start = datetime.combine(window_start_date, time(9, 30), timestamp.tzinfo)\n",
    "    window_end = timestamp\n",
    "    # filename = f\"/Volumes/WD18TB/chart_images/daily_chart_{timestamp}.png\"\n",
    "    filename = f\"/Users/brandon/Documents/chart_images/daily_chart_{timestamp}.png\"\n",
    "    generate_and_save_chart(df, 'NVDA', window_start, window_end, filename)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "def execute_concurrently(unique_timestamps, max_workers=4, clear_dir=False):\n",
    "    # Clear the chart_images directory if clear_dir is True\n",
    "    if clear_dir:\n",
    "        # dir_path = \"/Volumes/WD18TB/chart_images\"\n",
    "        dir_path = \"/Users/brandon/Documents/chart_images/\"\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Cleared and recreated directory: {dir_path}\")\n",
    "\n",
    "    # Execute the function concurrently for each timestamp\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        executor.map(process_timestamp, unique_timestamps)\n",
    "\n",
    "# Example usage with configurable workers\n",
    "max_workers = 8\n",
    "execute_concurrently(unique_timestamps, max_workers, clear_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bd974-25b5-4bfd-b5e9-b9dbaf9f8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "################MOVE files into timestamp specific directory\n",
    "\n",
    "import polars as pl\n",
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pytz\n",
    "\n",
    "# Load the indicators CSV\n",
    "# df = pl.read_csv(\"/mnt/data/indicators_240201_240330.csv\")\n",
    "\n",
    "# Ensure window_start_est is treated as EST\n",
    "df = df.with_columns(pl.col(\"window_start_est\").cast(pl.Datetime).dt.replace_time_zone(\"America/New_York\"))\n",
    "\n",
    "# Set the directory path\n",
    "base_dir = os.path.expanduser(\"/Users/brandon/Documents/chart_images/\")\n",
    "\n",
    "# Function to process each directory\n",
    "def process_directory(directory):\n",
    "    # Extract the timestamp from the directory name\n",
    "    timestamp_str = directory  # directory name is already in the correct format YYYYMMDDHHMMSS\n",
    "    timestamp = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "    est = pytz.timezone('America/New_York')\n",
    "    timestamp_est = est.localize(timestamp)\n",
    "    \n",
    "    # Calculate the date range\n",
    "    start_date = timestamp_est - timedelta(days=20)\n",
    "    end_date = timestamp_est + timedelta(days=20)\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df.filter((pl.col(\"window_start_est\") >= start_date) & (pl.col(\"window_start_est\") <= end_date))\n",
    "    \n",
    "    # Save to CSV in the respective directory\n",
    "    output_path = os.path.join(base_dir, directory, f\"indicators_{timestamp_str}.csv\")\n",
    "    filtered_df.write_csv(output_path)\n",
    "    print(f\"Saved CSV for {timestamp_str}\")\n",
    "\n",
    "# Get the list of directories\n",
    "directories = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d)) and d.isdigit()]\n",
    "\n",
    "# Use ThreadPoolExecutor to process directories in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Map the function to the directories\n",
    "    list(executor.map(process_directory, directories))\n",
    "\n",
    "print(\"All directories processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b807687-d515-4b37-be47-fafe24c9565e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:54:00-05:00.pngProcessing: 2024-02-01 13:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:55:00-05:00.png\n",
      "Processing: 2024-02-01 13:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:56:00-05:00.png\n",
      "Processing: 2024-02-01 13:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:57:00-05:00.png\n",
      "Processing: 2024-02-01 13:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:58:00-05:00.png\n",
      "Processing: 2024-02-01 13:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 12:59:00-05:00.png\n",
      "Processing: 2024-02-01 13:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:00:00-05:00.png\n",
      "Processing: 2024-02-01 13:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:01:00-05:00.png\n",
      "Processing: 2024-02-01 13:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:02:00-05:00.png\n",
      "Processing: 2024-02-01 13:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:03:00-05:00.png\n",
      "Processing: 2024-02-01 13:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:04:00-05:00.png\n",
      "Processing: 2024-02-01 13:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:05:00-05:00.png\n",
      "Processing: 2024-02-01 13:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:06:00-05:00.png\n",
      "Processing: 2024-02-01 13:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:07:00-05:00.png\n",
      "Processing: 2024-02-01 13:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:08:00-05:00.png\n",
      "Processing: 2024-02-01 13:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:09:00-05:00.png\n",
      "Processing: 2024-02-01 13:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:10:00-05:00.png\n",
      "Processing: 2024-02-01 13:18:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:11:00-05:00.png\n",
      "Processing: 2024-02-01 13:19:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:12:00-05:00.png\n",
      "Processing: 2024-02-01 13:20:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:13:00-05:00.png\n",
      "Processing: 2024-02-01 13:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:14:00-05:00.png\n",
      "Processing: 2024-02-01 13:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:15:00-05:00.png\n",
      "Processing: 2024-02-01 13:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:16:00-05:00.png\n",
      "Processing: 2024-02-01 13:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:18:00-05:00.png\n",
      "Processing: 2024-02-01 13:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:17:00-05:00.png\n",
      "Processing: 2024-02-01 13:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:19:00-05:00.png\n",
      "Processing: 2024-02-01 13:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:20:00-05:00.png\n",
      "Processing: 2024-02-01 13:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:21:00-05:00.png\n",
      "Processing: 2024-02-01 13:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:22:00-05:00.png\n",
      "Processing: 2024-02-01 13:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:23:00-05:00.png\n",
      "Processing: 2024-02-01 13:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:24:00-05:00.png\n",
      "Processing: 2024-02-01 13:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:26:00-05:00.png\n",
      "Processing: 2024-02-01 13:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:25:00-05:00.png\n",
      "Processing: 2024-02-01 13:34:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:27:00-05:00.png\n",
      "Processing: 2024-02-01 13:35:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:28:00-05:00.png\n",
      "Processing: 2024-02-01 13:36:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:29:00-05:00.png\n",
      "Processing: 2024-02-01 13:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:30:00-05:00.png\n",
      "Processing: 2024-02-01 13:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:31:00-05:00.png\n",
      "Processing: 2024-02-01 13:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:32:00-05:00.png\n",
      "Processing: 2024-02-01 13:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:33:00-05:00.png\n",
      "Processing: 2024-02-01 13:41:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:34:00-05:00.pngProcessing: 2024-02-01 13:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:35:00-05:00.png\n",
      "Processing: 2024-02-01 13:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:36:00-05:00.png\n",
      "Processing: 2024-02-01 13:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:37:00-05:00.png\n",
      "Processing: 2024-02-01 13:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:39:00-05:00.png\n",
      "Processing: 2024-02-01 13:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:38:00-05:00.png\n",
      "Processing: 2024-02-01 13:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:41:00-05:00.png\n",
      "Processing: 2024-02-01 13:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:40:00-05:00.png\n",
      "Processing: 2024-02-01 13:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:42:00-05:00.png\n",
      "Processing: 2024-02-01 13:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:45:00-05:00.png\n",
      "Processing: 2024-02-01 13:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:44:00-05:00.png\n",
      "Processing: 2024-02-01 13:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:43:00-05:00.png\n",
      "Processing: 2024-02-01 13:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:47:00-05:00.png\n",
      "Processing: 2024-02-01 13:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:46:00-05:00.png\n",
      "Processing: 2024-02-01 13:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:48:00-05:00.png\n",
      "Processing: 2024-02-01 13:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:49:00-05:00.png\n",
      "Processing: 2024-02-01 13:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:50:00-05:00.png\n",
      "Processing: 2024-02-01 13:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:51:00-05:00.png\n",
      "Processing: 2024-02-01 13:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:52:00-05:00.png\n",
      "Processing: 2024-02-01 14:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:53:00-05:00.png\n",
      "Processing: 2024-02-01 14:01:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:54:00-05:00.png\n",
      "Processing: 2024-02-01 14:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:56:00-05:00.png\n",
      "Processing: 2024-02-01 14:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:55:00-05:00.png\n",
      "Processing: 2024-02-01 14:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:57:00-05:00.png\n",
      "Processing: 2024-02-01 14:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:58:00-05:00.png\n",
      "Processing: 2024-02-01 14:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 13:59:00-05:00.png\n",
      "Processing: 2024-02-01 14:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:00:00-05:00.png\n",
      "Processing: 2024-02-01 14:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:01:00-05:00.png\n",
      "Processing: 2024-02-01 14:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:03:00-05:00.png\n",
      "Processing: 2024-02-01 14:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:02:00-05:00.png\n",
      "Processing: 2024-02-01 14:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:04:00-05:00.png\n",
      "Processing: 2024-02-01 14:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:05:00-05:00.png\n",
      "Processing: 2024-02-01 14:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:06:00-05:00.png\n",
      "Processing: 2024-02-01 14:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:08:00-05:00.png\n",
      "Processing: 2024-02-01 14:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:07:00-05:00.png\n",
      "Processing: 2024-02-01 14:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:09:00-05:00.png\n",
      "Processing: 2024-02-01 14:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:10:00-05:00.png\n",
      "Processing: 2024-02-01 14:18:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:11:00-05:00.png\n",
      "Processing: 2024-02-01 14:19:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:12:00-05:00.png\n",
      "Processing: 2024-02-01 14:20:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:14:00-05:00.png\n",
      "Processing: 2024-02-01 14:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:13:00-05:00.png\n",
      "Processing: 2024-02-01 14:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:15:00-05:00.png\n",
      "Processing: 2024-02-01 14:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:16:00-05:00.png\n",
      "Processing: 2024-02-01 14:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:17:00-05:00.png\n",
      "Processing: 2024-02-01 14:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:18:00-05:00.png\n",
      "Processing: 2024-02-01 14:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:20:00-05:00.png\n",
      "Processing: 2024-02-01 14:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:19:00-05:00.png\n",
      "Processing: 2024-02-01 14:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:22:00-05:00.png\n",
      "Processing: 2024-02-01 14:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:21:00-05:00.png\n",
      "Processing: 2024-02-01 14:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:24:00-05:00.png\n",
      "Processing: 2024-02-01 14:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:23:00-05:00.png\n",
      "Processing: 2024-02-01 14:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:25:00-05:00.png\n",
      "Processing: 2024-02-01 14:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:26:00-05:00.png\n",
      "Processing: 2024-02-01 14:34:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:27:00-05:00.png\n",
      "Processing: 2024-02-01 14:35:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:28:00-05:00.png\n",
      "Processing: 2024-02-01 14:36:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:30:00-05:00.png\n",
      "Processing: 2024-02-01 14:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:29:00-05:00.png\n",
      "Processing: 2024-02-01 14:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:31:00-05:00.png\n",
      "Processing: 2024-02-01 14:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:32:00-05:00.png\n",
      "Processing: 2024-02-01 14:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:34:00-05:00.png\n",
      "Processing: 2024-02-01 14:41:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:33:00-05:00.png\n",
      "Processing: 2024-02-01 14:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:35:00-05:00.png\n",
      "Processing: 2024-02-01 14:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:36:00-05:00.png\n",
      "Processing: 2024-02-01 14:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:38:00-05:00.png\n",
      "Processing: 2024-02-01 14:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:37:00-05:00.png\n",
      "Processing: 2024-02-01 14:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:40:00-05:00.png\n",
      "Processing: 2024-02-01 14:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:39:00-05:00.png\n",
      "Processing: 2024-02-01 14:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:42:00-05:00.png\n",
      "Processing: 2024-02-01 14:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:41:00-05:00.png\n",
      "Processing: 2024-02-01 14:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:43:00-05:00.png\n",
      "Processing: 2024-02-01 14:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:44:00-05:00.png\n",
      "Processing: 2024-02-01 14:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:45:00-05:00.png\n",
      "Processing: 2024-02-01 14:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:46:00-05:00.png\n",
      "Processing: 2024-02-01 14:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:47:00-05:00.png\n",
      "Processing: 2024-02-01 14:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:49:00-05:00.png\n",
      "Processing: 2024-02-01 14:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:48:00-05:00.png\n",
      "Processing: 2024-02-01 14:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:51:00-05:00.png\n",
      "Processing: 2024-02-01 14:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:50:00-05:00.png\n",
      "Processing: 2024-02-01 14:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:52:00-05:00.png\n",
      "Processing: 2024-02-01 15:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:54:00-05:00.png\n",
      "Processing: 2024-02-01 15:01:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:53:00-05:00.png\n",
      "Processing: 2024-02-01 15:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:55:00-05:00.png\n",
      "Processing: 2024-02-01 15:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:56:00-05:00.png\n",
      "Processing: 2024-02-01 15:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:57:00-05:00.png\n",
      "Processing: 2024-02-01 15:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:58:00-05:00.png\n",
      "Processing: 2024-02-01 15:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 14:59:00-05:00.png\n",
      "Processing: 2024-02-01 15:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:01:00-05:00.png\n",
      "Processing: 2024-02-01 15:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:02:00-05:00.png\n",
      "Processing: 2024-02-01 15:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:00:00-05:00.png\n",
      "Processing: 2024-02-01 15:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:04:00-05:00.png\n",
      "Processing: 2024-02-01 15:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:03:00-05:00.png\n",
      "Processing: 2024-02-01 15:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:05:00-05:00.png\n",
      "Processing: 2024-02-01 15:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:06:00-05:00.png\n",
      "Processing: 2024-02-01 15:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:07:00-05:00.png\n",
      "Processing: 2024-02-01 15:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:08:00-05:00.png\n",
      "Processing: 2024-02-01 15:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:09:00-05:00.png\n",
      "Processing: 2024-02-01 15:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:10:00-05:00.png\n",
      "Processing: 2024-02-01 15:18:00-05:00\n",
      "Hourly counts before timezone conversion:\n",
      "window_start_est\n",
      "9     39498\n",
      "10    79080\n",
      "11    79079\n",
      "12    79077\n",
      "13    78783\n",
      "14    78683\n",
      "15    78670\n",
      "16     1312\n",
      "Name: count, dtype: int64\n",
      "Hourly counts after timezone conversion:\n",
      "window_start_est\n",
      "9     39498\n",
      "10    79080\n",
      "11    79079\n",
      "12    79077\n",
      "13    78783\n",
      "14    78683\n",
      "15    78670\n",
      "16     1312\n",
      "Name: count, dtype: int64\n",
      "Full range of 'window_start_est':\n",
      "Min: 2019-03-18 09:30:00-04:00\n",
      "Max: 2024-06-10 16:00:00-04:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:12:00-05:00.png\n",
      "Processing: 2024-02-01 15:19:00-05:00\n",
      "Filtered hourly counts:\n",
      "window_start_est\n",
      "9     30\n",
      "10    60\n",
      "11    60\n",
      "12    60\n",
      "13    60\n",
      "14    60\n",
      "15    60\n",
      "16     1\n",
      "Name: count, dtype: int64\n",
      "Filtered data range:\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:11:00-05:00.png\n",
      "Processing: 2024-02-01 15:20:00-05:00\n",
      "       adjusted_volume\n",
      "count     3.910000e+02\n",
      "mean      1.126585e+06\n",
      "std       7.961444e+05\n",
      "min       2.901500e+05\n",
      "25%       6.348050e+05\n",
      "50%       8.819900e+05\n",
      "75%       1.356700e+06\n",
      "max       7.111480e+06\n",
      "2024-02-02 09:30:00-05:00\n",
      "2024-02-02 16:00:00-05:00\n",
      "                window_start_est        rsi\n",
      "479383 2024-02-02 09:30:00-05:00  81.625272\n",
      "479384 2024-02-02 09:31:00-05:00  86.444257\n",
      "479385 2024-02-02 09:32:00-05:00  84.858886\n",
      "479386 2024-02-02 09:33:00-05:00  87.501797\n",
      "479387 2024-02-02 09:34:00-05:00  89.776030\n",
      "479388 2024-02-02 09:35:00-05:00  91.083843\n",
      "479389 2024-02-02 09:36:00-05:00  93.104459\n",
      "479390 2024-02-02 09:37:00-05:00  93.884279\n",
      "479391 2024-02-02 09:38:00-05:00  92.893733\n",
      "479392 2024-02-02 09:39:00-05:00  89.813702\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:13:00-05:00.png\n",
      "Processing: 2024-02-01 15:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:14:00-05:00.png\n",
      "Processing: 2024-02-01 15:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:15:00-05:00.png\n",
      "Processing: 2024-02-01 15:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:16:00-05:00.png\n",
      "Processing: 2024-02-01 15:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:18:00-05:00.png\n",
      "Processing: 2024-02-01 15:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:19:00-05:00.png\n",
      "Processing: 2024-02-01 15:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:17:00-05:00.png\n",
      "Processing: 2024-02-01 15:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:20:00-05:00.png\n",
      "Processing: 2024-02-01 15:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:21:00-05:00.png\n",
      "Processing: 2024-02-01 15:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:22:00-05:00.png\n",
      "Processing: 2024-02-01 15:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:23:00-05:00.png\n",
      "Processing: 2024-02-01 15:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:25:00-05:00.png\n",
      "Processing: 2024-02-01 15:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:24:00-05:00.png\n",
      "Processing: 2024-02-01 15:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:27:00-05:00.png\n",
      "Processing: 2024-02-01 15:34:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:29:00-05:00.png\n",
      "Processing: 2024-02-01 15:35:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:26:00-05:00.png\n",
      "Processing: 2024-02-01 15:36:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:28:00-05:00.png\n",
      "Processing: 2024-02-01 15:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:31:00-05:00.png\n",
      "Processing: 2024-02-01 15:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:30:00-05:00.png\n",
      "Processing: 2024-02-01 15:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:33:00-05:00.pngProcessing: 2024-02-01 15:40:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:32:00-05:00.png\n",
      "Processing: 2024-02-01 15:41:00-05:00\n"
     ]
    }
   ],
   "source": [
    "# Chart with Support/Resistance Channels\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "ticker = 'NVDA'\n",
    "\n",
    "# Convert the Polars DataFrame to pandas\n",
    "pandas_df = df.filter((pl.col('ticker') == ticker)).to_pandas()\n",
    "pandas_df['window_start_est'] = pd.to_datetime(pandas_df['window_start_est'])\n",
    "\n",
    "# Print the hourly counts before timezone conversion\n",
    "hourly_counts = pandas_df['window_start_est'].dt.hour.value_counts().sort_index()\n",
    "print(\"Hourly counts before timezone conversion:\")\n",
    "print(hourly_counts)\n",
    "\n",
    "# Ensure the 'window_start_est' column is interpreted as UTC initially\n",
    "# pandas_df['window_start_est'] = pandas_df['window_start_est'].dt.tz_localize('UTC')\n",
    "# pandas_df['window_start_est'] = pandas_df['window_start_est'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "# Properly convert the 'window_start_est' column to US/Eastern\n",
    "pandas_df['window_start_est'] = pandas_df['window_start_est'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "# Print the hourly counts after timezone conversion\n",
    "hourly_counts = pandas_df['window_start_est'].dt.hour.value_counts().sort_index()\n",
    "print(\"Hourly counts after timezone conversion:\")\n",
    "print(hourly_counts)\n",
    "\n",
    "# Verify the full range of 'window_start_est' column\n",
    "print(\"Full range of 'window_start_est':\")\n",
    "print(\"Min:\", pandas_df['window_start_est'].min())\n",
    "print(\"Max:\", pandas_df['window_start_est'].max())\n",
    "\n",
    "# Filter the DataFrame to only include data within the specified window\n",
    "window_start = Timestamp('2024-05-15 09:30', tz='US/Eastern')\n",
    "window_end = Timestamp('2024-05-15 16:00', tz='US/Eastern')\n",
    "\n",
    "window_start = Timestamp('2024-02-02 09:30', tz='US/Eastern')\n",
    "window_end = Timestamp('2024-02-02 16:00', tz='US/Eastern')\n",
    "\n",
    "filtered_df = pandas_df[\n",
    "    (pandas_df['window_start_est'] >= window_start) &\n",
    "    (pandas_df['window_start_est'] <= window_end)\n",
    "]\n",
    "\n",
    "# Print the hourly counts for the filtered data\n",
    "hourly_counts_filtered = filtered_df['window_start_est'].dt.hour.value_counts().sort_index()\n",
    "print(\"Filtered hourly counts:\")\n",
    "print(hourly_counts_filtered)\n",
    "\n",
    "# Verify the data range after filtering\n",
    "print(\"Filtered data range:\")\n",
    "print(filtered_df[['window_start_est', 'adjusted_volume']].describe())\n",
    "print(filtered_df['window_start_est'].min())\n",
    "print(filtered_df['window_start_est'].max())\n",
    "\n",
    "# Check if RSI has been calculated correctly\n",
    "print(filtered_df[['window_start_est', 'rsi']].dropna().head(10))\n",
    "\n",
    "# Determine crossing points\n",
    "cross_above = filtered_df[filtered_df['MACD_cross_above_Signal'] == 1]['window_start_est']\n",
    "cross_below = filtered_df[filtered_df['MACD_cross_below_Signal'] == 1]['window_start_est']\n",
    "\n",
    "# Create a subplot with 4 rows\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1, shared_xaxes=True, vertical_spacing=0.1,  # Increased spacing\n",
    "    row_heights=[0.5, 0.2, 0.15, 0.15],  # Adjust the heights of the subplots\n",
    "    subplot_titles=(\n",
    "        f'{ticker} Stock Price with 2-day and 3-day VWAP',\n",
    "        'MACD & Signal Line',\n",
    "        'adjusted_volume',\n",
    "        'RSI'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add the candlestick chart to the first row\n",
    "fig.add_trace(\n",
    "    go.Candlestick(\n",
    "        x=filtered_df['window_start_est'], open=filtered_df['adjusted_open'],\n",
    "        high=filtered_df['adjusted_high'], low=filtered_df['adjusted_low'],\n",
    "        close=filtered_df['adjusted_close']\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adding the 2-day VWAP to the candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['vwap_2d'],\n",
    "        mode='lines', name='2-day VWAP', line=dict(width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adding the 3-day VWAP to the candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['vwap_3d'],\n",
    "        mode='lines', name='3-day VWAP', line=dict(width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adding the ema_200 to the candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['ema_200_vwap'],\n",
    "        mode='lines', name='EMA 200', line=dict(width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add support and resistance lines to the candlestick chart\n",
    "support_levels = filtered_df['support_level'].dropna().unique()\n",
    "resistance_levels = filtered_df['resistance_level'].dropna().unique()\n",
    "\n",
    "for level in support_levels:\n",
    "    fig.add_hline(y=level, line=dict(color=\"green\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "for level in resistance_levels:\n",
    "    fig.add_hline(y=level, line=dict(color=\"red\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "# Add MACD line to the second row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['MACD_line'],\n",
    "        mode='lines', name='MACD Line'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add Signal line to the second row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['signal_line'],\n",
    "        mode='lines', name='Signal Line'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add vertical lines for MACD crossing above Signal line\n",
    "for cross_time in cross_above:\n",
    "    fig.add_vline(\n",
    "        x=cross_time, line=dict(color=\"green\", width=2),\n",
    "        row='all', col=1\n",
    "    )\n",
    "\n",
    "# Add vertical lines for MACD crossing below Signal line\n",
    "for cross_time in cross_below:\n",
    "    fig.add_vline(\n",
    "        x=cross_time, line=dict(color=\"red\", width=2),\n",
    "        row='all', col=1\n",
    "    )\n",
    "\n",
    "# Add MACD histogram to the second row (below MACD and Signal Line)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=filtered_df['window_start_est'],\n",
    "        y=filtered_df['MACD_histogram'],\n",
    "        name='MACD Histogram',\n",
    "        marker_color=np.where(filtered_df['MACD_histogram'] >= 0, 'green', 'red')  # color bars conditionally\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add horizontal line at y=0 in the MACD plot\n",
    "fig.add_hline(y=0, line=dict(color=\"black\", width=2, dash=\"solid\"), row=2, col=1)\n",
    "\n",
    "# Add volume bars to the third row\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=filtered_df['window_start_est'],\n",
    "        y=filtered_df['adjusted_volume'],\n",
    "        marker_color='blue',\n",
    "        name='adjusted_volume'\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# Add RSI line to the fourth row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['rsi'],\n",
    "        mode='lines', name='RSI'\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "# Add horizontal lines at RSI levels 30 and 70\n",
    "fig.add_hline(y=30, line=dict(color=\"red\", width=1, dash=\"dash\"), row=4, col=1)\n",
    "fig.add_hline(y=70, line=dict(color=\"red\", width=1, dash=\"dash\"), row=4, col=1)\n",
    "\n",
    "# Updating layout for better readability\n",
    "fig.update_layout(\n",
    "    title=f'{ticker} Stock Price and Technical Indicators on {window_start.strftime(\"%Y-%m-%d %H:%M %Z\")}',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Price (USD)',\n",
    "    legend_title_text='Indicator',\n",
    "    xaxis_rangeslider_visible=False\n",
    ")\n",
    "\n",
    "# Adjust x-axis and y-axis titles for subplots\n",
    "fig.update_xaxes(title_text=\"Time\", row=1, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=2, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=3, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=4, col=1, tickformat=\"%H:%M\")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Price (USD)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"MACD\", row=2, col=1)\n",
    "fig.update_yaxes(title_text='adjusted_volume', row=3, col=1)\n",
    "\n",
    "# Show the plot\n",
    "pio.renderers.default = \"notebook\"\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b29868fc-c88a-4958-8cf2-fa9e3e097576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:35:00-05:00.png\n",
      "Processing: 2024-02-01 15:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:34:00-05:00.png\n",
      "Processing: 2024-02-01 15:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:37:00-05:00.png\n",
      "Processing: 2024-02-01 15:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:36:00-05:00.png\n",
      "Processing: 2024-02-01 15:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:39:00-05:00.png\n",
      "Processing: 2024-02-01 15:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:38:00-05:00.png\n",
      "Processing: 2024-02-01 15:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:40:00-05:00.pngProcessing: 2024-02-01 15:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:41:00-05:00.png\n",
      "Processing: 2024-02-01 15:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:42:00-05:00.png\n",
      "Processing: 2024-02-01 15:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:43:00-05:00.png\n",
      "Processing: 2024-02-01 15:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:44:00-05:00.png\n",
      "Processing: 2024-02-01 15:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:45:00-05:00.png\n",
      "Processing: 2024-02-01 15:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:47:00-05:00.png\n",
      "Processing: 2024-02-01 15:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:46:00-05:00.png\n",
      "Processing: 2024-02-01 15:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:48:00-05:00.png\n",
      "Processing: 2024-02-01 15:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:49:00-05:00.png\n",
      "Processing: 2024-02-01 15:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:50:00-05:00.png\n",
      "Processing: 2024-02-01 15:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:52:00-05:00.png\n",
      "Processing: 2024-02-01 15:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:51:00-05:00.png\n",
      "Processing: 2024-02-01 16:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:53:00-05:00.png\n",
      "Processing: 2024-02-02 09:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:54:00-05:00.png\n",
      "Processing: 2024-02-02 09:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:55:00-05:00.png\n",
      "Processing: 2024-02-02 09:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:56:00-05:00.png\n",
      "Processing: 2024-02-02 09:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:57:00-05:00.png\n",
      "Processing: 2024-02-02 09:34:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:30:00-05:00.png\n",
      "Processing: 2024-02-02 09:35:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:31:00-05:00.png\n",
      "Processing: 2024-02-02 09:36:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:32:00-05:00.png\n",
      "Processing: 2024-02-02 09:37:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:34:00-05:00.png\n",
      "Processing: 2024-02-02 09:38:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:33:00-05:00.png\n",
      "Processing: 2024-02-02 09:39:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 16:00:00-05:00.png\n",
      "Processing: 2024-02-02 09:40:00-05:00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Already tz-aware, use tz_convert to convert.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_start_est\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_start_est\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Ensure the 'window_start_est' column is interpreted as UTC initially\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_start_est\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwindow_start_est\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz_localize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUTC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Properly convert the 'window_start_est' column to US/Eastern\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_start_est\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pandas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_start_est\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz_convert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUS/Eastern\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/accessor.py:112\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_method.<locals>.f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delegate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/accessors.py:134\u001b[0m, in \u001b[0;36mProperties._delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m    133\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m--> 134\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(result):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/datetimes.py:291\u001b[0m, in \u001b[0;36mDatetimeIndex.tz_localize\u001b[0;34m(self, tz, ambiguous, nonexistent)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;129m@doc\u001b[39m(DatetimeArray\u001b[38;5;241m.\u001b[39mtz_localize)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtz_localize\u001b[39m(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     nonexistent: TimeNonexistent \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 291\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz_localize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonexistent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_simple_new(arr, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/_mixins.py:80\u001b[0m, in \u001b[0;36mravel_compat.<locals>.method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(meth)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmethod\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray\u001b[38;5;241m.\u001b[39mflags\n\u001b[1;32m     83\u001b[0m     flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mravel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:1061\u001b[0m, in \u001b[0;36mDatetimeArray.tz_localize\u001b[0;34m(self, tz, ambiguous, nonexistent)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         new_dates \u001b[38;5;241m=\u001b[39m tz_convert_from_utc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masi8, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz, reso\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_creso)\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlready tz-aware, use tz_convert to convert.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     tz \u001b[38;5;241m=\u001b[39m timezones\u001b[38;5;241m.\u001b[39mmaybe_get_tz(tz)\n",
      "\u001b[0;31mTypeError\u001b[0m: Already tz-aware, use tz_convert to convert."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:59:00-05:00.png\n",
      "Processing: 2024-02-02 09:41:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-01 15:58:00-05:00.png\n",
      "Processing: 2024-02-02 09:42:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:35:00-05:00.png\n",
      "Processing: 2024-02-02 09:43:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:36:00-05:00.png\n",
      "Processing: 2024-02-02 09:44:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:37:00-05:00.png\n",
      "Processing: 2024-02-02 09:45:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:38:00-05:00.png\n",
      "Processing: 2024-02-02 09:46:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:39:00-05:00.pngProcessing: 2024-02-02 09:47:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:40:00-05:00.png\n",
      "Processing: 2024-02-02 09:48:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:41:00-05:00.png\n",
      "Processing: 2024-02-02 09:49:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:42:00-05:00.png\n",
      "Processing: 2024-02-02 09:50:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:43:00-05:00.png\n",
      "Processing: 2024-02-02 09:51:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:44:00-05:00.png\n",
      "Processing: 2024-02-02 09:52:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:45:00-05:00.png\n",
      "Processing: 2024-02-02 09:53:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:46:00-05:00.png\n",
      "Processing: 2024-02-02 09:54:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:47:00-05:00.png\n",
      "Processing: 2024-02-02 09:55:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:48:00-05:00.png\n",
      "Processing: 2024-02-02 09:56:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:49:00-05:00.png\n",
      "Processing: 2024-02-02 09:57:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:50:00-05:00.png\n",
      "Processing: 2024-02-02 09:58:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:51:00-05:00.png\n",
      "Processing: 2024-02-02 09:59:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:52:00-05:00.png\n",
      "Processing: 2024-02-02 10:00:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:53:00-05:00.png\n",
      "Processing: 2024-02-02 10:01:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:54:00-05:00.png\n",
      "Processing: 2024-02-02 10:02:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:55:00-05:00.png\n",
      "Processing: 2024-02-02 10:03:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:56:00-05:00.png\n",
      "Processing: 2024-02-02 10:04:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:57:00-05:00.png\n",
      "Processing: 2024-02-02 10:05:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:58:00-05:00.png\n",
      "Processing: 2024-02-02 10:06:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 09:59:00-05:00.png\n",
      "Processing: 2024-02-02 10:07:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:00:00-05:00.png\n",
      "Processing: 2024-02-02 10:08:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:01:00-05:00.png\n",
      "Processing: 2024-02-02 10:09:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:02:00-05:00.png\n",
      "Processing: 2024-02-02 10:10:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:03:00-05:00.png\n",
      "Processing: 2024-02-02 10:11:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:04:00-05:00.png\n",
      "Processing: 2024-02-02 10:12:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:05:00-05:00.png\n",
      "Processing: 2024-02-02 10:13:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:06:00-05:00.png\n",
      "Processing: 2024-02-02 10:14:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:07:00-05:00.png\n",
      "Processing: 2024-02-02 10:15:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:08:00-05:00.png\n",
      "Processing: 2024-02-02 10:16:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:09:00-05:00.png\n",
      "Processing: 2024-02-02 10:17:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:10:00-05:00.png\n",
      "Processing: 2024-02-02 10:18:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:11:00-05:00.png\n",
      "Processing: 2024-02-02 10:19:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:12:00-05:00.png\n",
      "Processing: 2024-02-02 10:20:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:13:00-05:00.png\n",
      "Processing: 2024-02-02 10:21:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:14:00-05:00.png\n",
      "Processing: 2024-02-02 10:22:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:15:00-05:00.png\n",
      "Processing: 2024-02-02 10:23:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:16:00-05:00.png\n",
      "Processing: 2024-02-02 10:24:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:17:00-05:00.png\n",
      "Processing: 2024-02-02 10:25:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:18:00-05:00.png\n",
      "Processing: 2024-02-02 10:26:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:19:00-05:00.png\n",
      "Processing: 2024-02-02 10:27:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:20:00-05:00.png\n",
      "Processing: 2024-02-02 10:28:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:21:00-05:00.png\n",
      "Processing: 2024-02-02 10:29:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:22:00-05:00.png\n",
      "Processing: 2024-02-02 10:30:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:23:00-05:00.png\n",
      "Processing: 2024-02-02 10:31:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:24:00-05:00.png\n",
      "Processing: 2024-02-02 10:32:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:25:00-05:00.png\n",
      "Processing: 2024-02-02 10:33:00-05:00\n",
      "Saved /Volumes/WD18TB/chart_images/daily_chart_2024-02-02 10:26:00-05:00.png\n",
      "Processing: 2024-02-02 10:34:00-05:00\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "ticker = 'NVDA'\n",
    "\n",
    "# Convert the Polars DataFrame to pandas\n",
    "pandas_df = df.filter((pl.col('ticker') == ticker)).to_pandas()\n",
    "pandas_df['window_start_est'] = pd.to_datetime(pandas_df['window_start_est'])\n",
    "\n",
    "# Ensure the 'window_start_est' column is interpreted as UTC initially\n",
    "pandas_df['window_start_est'] = pandas_df['window_start_est'].dt.tz_localize('UTC')\n",
    "\n",
    "# Properly convert the 'window_start_est' column to US/Eastern\n",
    "pandas_df['window_start_est'] = pandas_df['window_start_est'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "# Filter the DataFrame to only include data within the specified window\n",
    "window_start = Timestamp('2024-05-15 09:30', tz='US/Eastern')\n",
    "window_end = Timestamp('2024-05-15 16:00', tz='US/Eastern')\n",
    "window_start = Timestamp('2024-05-07 09:30', tz='US/Eastern')\n",
    "window_end = Timestamp('2024-05-07 16:00', tz='US/Eastern')\n",
    "window_start = Timestamp('2024-02-01 09:30', tz='US/Eastern')\n",
    "window_end = Timestamp('2024-02-01 16:00', tz='US/Eastern')\n",
    "\n",
    "filtered_df = pandas_df[\n",
    "    (pandas_df['window_start_est'] >= window_start) &\n",
    "    (pandas_df['window_start_est'] <= window_end)\n",
    "]\n",
    "\n",
    "# Determine crossing points\n",
    "cross_above = filtered_df[filtered_df['MACD_cross_above_Signal'] == 1]['window_start_est']\n",
    "cross_below = filtered_df[filtered_df['MACD_cross_below_Signal'] == 1]['window_start_est']\n",
    "\n",
    "# Create a subplot with 5 rows\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1, shared_xaxes=True, vertical_spacing=0.1,  # Increased spacing\n",
    "    row_heights=[0.4, 0.2, 0.15, 0.15, 0.1],  # Adjust the heights of the subplots\n",
    "    subplot_titles=(\n",
    "        f'{ticker} Stock Price with 2-day and 3-day VWAP',\n",
    "        'MACD & Signal Line',\n",
    "        'adjusted_volume',\n",
    "        'RSI',\n",
    "        'Hammer Candlesticks'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add the candlestick chart to the first row\n",
    "fig.add_trace(\n",
    "    go.Candlestick(\n",
    "        x=filtered_df['window_start_est'], open=filtered_df['adjusted_open'],\n",
    "        high=filtered_df['adjusted_high'], low=filtered_df['adjusted_low'],\n",
    "        close=filtered_df['adjusted_close']\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adding the 2-day VWAP to the candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['vwap_2d'],\n",
    "        mode='lines', name='2-day VWAP', line=dict(width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adding the 3-day VWAP to the candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['vwap_3d'],\n",
    "        mode='lines', name='3-day VWAP', line=dict(width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Adding the ema_200 to the candlestick chart\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['ema_200_vwap'],\n",
    "        mode='lines', name='EMA 200', line=dict(width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add support and resistance lines to the candlestick chart\n",
    "support_levels = filtered_df['support_level'].dropna().unique()\n",
    "resistance_levels = filtered_df['resistance_level'].dropna().unique()\n",
    "\n",
    "for level in support_levels:\n",
    "    fig.add_hline(y=level, line=dict(color=\"green\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "for level in resistance_levels:\n",
    "    fig.add_hline(y=level, line=dict(color=\"red\", width=1, dash=\"dash\"), row=1, col=1)\n",
    "\n",
    "# Add MACD line to the second row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['MACD_line'],\n",
    "        mode='lines', name='MACD Line'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add Signal line to the second row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['signal_line'],\n",
    "        mode='lines', name='Signal Line'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add vertical lines for MACD crossing above Signal line\n",
    "for cross_time in cross_above:\n",
    "    fig.add_vline(\n",
    "        x=cross_time, line=dict(color=\"green\", width=2),\n",
    "        row='all', col=1\n",
    "    )\n",
    "\n",
    "# Add vertical lines for MACD crossing below Signal line\n",
    "for cross_time in cross_below:\n",
    "    fig.add_vline(\n",
    "        x=cross_time, line=dict(color=\"red\", width=2),\n",
    "        row='all', col=1\n",
    "    )\n",
    "\n",
    "# Add MACD histogram to the second row (below MACD and Signal Line)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=filtered_df['window_start_est'],\n",
    "        y=filtered_df['MACD_histogram'],\n",
    "        name='MACD Histogram',\n",
    "        marker_color=np.where(filtered_df['MACD_histogram'] >= 0, 'green', 'red')  # color bars conditionally\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add horizontal line at y=0 in the MACD plot\n",
    "fig.add_hline(y=0, line=dict(color=\"black\", width=2, dash=\"solid\"), row=2, col=1)\n",
    "\n",
    "# Add volume bars to the third row\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=filtered_df['window_start_est'],\n",
    "        y=filtered_df['adjusted_volume'],\n",
    "        marker_color='blue',\n",
    "        name='adjusted_volume'\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# Add RSI line to the fourth row\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=filtered_df['window_start_est'], y=filtered_df['rsi'],\n",
    "        mode='lines', name='RSI'\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "# Add horizontal lines at RSI levels 30 and 70\n",
    "fig.add_hline(y=30, line=dict(color=\"red\", width=1, dash=\"dash\"), row=4, col=1)\n",
    "fig.add_hline(y=70, line=dict(color=\"red\", width=1, dash=\"dash\"), row=4, col=1)\n",
    "\n",
    "# Create a DataFrame for hammer candles\n",
    "hammer_df = filtered_df[filtered_df['skrong_bullish_hammer'] == True]\n",
    "\n",
    "# Add hammer candlestick chart to the fifth row\n",
    "# fig.add_trace(\n",
    "#     go.Candlestick(\n",
    "#         x=hammer_df['window_start_est'], open=hammer_df['adjusted_open'],\n",
    "#         high=hammer_df['adjusted_high'], low=hammer_df['adjusted_low'],\n",
    "#         close=hammer_df['adjusted_close'],\n",
    "#         increasing_line_color='purple', decreasing_line_color='purple',\n",
    "#         name='Hammer Candlestick'\n",
    "#     ),\n",
    "#     row=5, col=1\n",
    "# )\n",
    "\n",
    "# Updating layout for better readability\n",
    "fig.update_layout(\n",
    "    title=f'{ticker} Stock Price and Technical Indicators on {window_start.strftime(\"%Y-%m-%d %H:%M %Z\")}',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Price (USD)',\n",
    "    legend_title_text='Indicator',\n",
    "    xaxis_rangeslider_visible=False\n",
    ")\n",
    "\n",
    "# Adjust x-axis and y-axis titles for subplots\n",
    "fig.update_xaxes(title_text=\"Time\", row=1, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=2, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=3, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=4, col=1, tickformat=\"%H:%M\")\n",
    "fig.update_xaxes(title_text=\"Time\", row=5, col=1, tickformat=\"%H:%M\")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Price (USD)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"MACD\", row=2, col=1)\n",
    "fig.update_yaxes(title_text='adjusted_volume', row=3, col=1)\n",
    "fig.update_yaxes(title_text='RSI', row=4, col=1)\n",
    "fig.update_yaxes(title_text='Price (USD)', row=5, col=1)\n",
    "\n",
    "# Show the plot\n",
    "pio.renderers.default = \"notebook\"\n",
    "fig.show(renderer='browser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ceec74-1b46-4a52-880d-47ab2386ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######ADD OPTIONS DATA\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "# Directory path where the CSV files are downloaded\n",
    "directory_path = \"/Volumes/WD18TB/us_options/minute_aggs/2024/05/\"\n",
    "\n",
    "# List all CSV files in the directory that end with \"14.csv.gz\"\n",
    "# files = [file for file in os.listdir(directory_path) if file.endswith(\"15.csv.gz\")]\n",
    "files = [file for file in os.listdir(directory_path)]\n",
    "print(files)\n",
    "\n",
    "def apply_adjustments(df: pl.DataFrame, splits_df: pl.DataFrame, type: str, price_cols: list, volume_cols: list) -> pl.DataFrame:\n",
    "    # Ensure splits_df is sorted by execution_date\n",
    "    splits_df = splits_df.sort('execution_date')\n",
    "\n",
    "    # Determine the prefix and ticker column based on the type\n",
    "    prefix = \"adjusted_\" if type == \"stock\" else \"adjusted_option_\"\n",
    "    ticker_col = \"ticker\" if type == \"stock\" else \"symbol\"\n",
    "\n",
    "    # Initialize the applied_splits column as an empty list\n",
    "    df = df.with_columns([pl.lit(\"\").alias('applied_splits')])\n",
    "\n",
    "    # Create adjusted columns for price and volume\n",
    "    for col in price_cols + volume_cols:\n",
    "        df = df.with_columns([pl.col(col).alias(f'{prefix}{col}')])\n",
    "\n",
    "    for split in splits_df.to_dicts():\n",
    "        ticker = split['ticker']\n",
    "        execution_date = dt.datetime.strptime(split['execution_date'], '%Y-%m-%d')\n",
    "        split_from = split['split_from']\n",
    "        split_to = split['split_to']\n",
    "        ratio = split_to / split_from\n",
    "\n",
    "        # Convert the split dictionary to a string for storage\n",
    "        split_str = str(split)\n",
    "\n",
    "        # Apply split to relevant rows\n",
    "        adjustments = []\n",
    "        for col in price_cols:\n",
    "            adjustments.append(\n",
    "                pl.when((pl.col(ticker_col) == ticker) & (pl.col('window_start_est') < execution_date))\n",
    "                .then(pl.col(f'{prefix}{col}') / ratio)\n",
    "                .otherwise(pl.col(f'{prefix}{col}')).alias(f'{prefix}{col}')\n",
    "            )\n",
    "        \n",
    "        for col in volume_cols:\n",
    "            adjustments.append(\n",
    "                pl.when((pl.col(ticker_col) == ticker) & (pl.col('window_start_est') < execution_date))\n",
    "                .then(pl.col(f'{prefix}{col}') * ratio)\n",
    "                .otherwise(pl.col(f'{prefix}{col}')).alias(f'{prefix}{col}')\n",
    "            )\n",
    "\n",
    "        adjustments.append(\n",
    "            pl.when((pl.col(ticker_col) == ticker) & (pl.col('window_start_est') < execution_date))\n",
    "            .then(pl.concat_str([pl.col('applied_splits'), pl.lit(f\",{split_str}\")]))\n",
    "            .otherwise(pl.col('applied_splits')).alias('applied_splits')\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(adjustments)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file and append its DataFrame to the list\n",
    "for file in files:\n",
    "    op_df = pl.read_csv(os.path.join(directory_path, file))\n",
    "    dfs.append(op_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "option_df = pl.concat(dfs)\n",
    "\n",
    "# Filter the DataFrame for a specific substring in the ticker column\n",
    "option_df = option_df.filter(pl.col(\"ticker\").str.contains(\"NVDA\"))  # Replace \"NVDA\" with the desired substring\n",
    "\n",
    "# Extract symbol, year, month, day, option_type, and strike_price\n",
    "option_df = option_df.with_columns([\n",
    "    pl.col(\"ticker\").str.slice(2, 4).alias(\"symbol\"),\n",
    "    (pl.col(\"ticker\").str.slice(6, 2).cast(pl.Int32) + 2000).cast(pl.Utf8).alias(\"year\"),\n",
    "    pl.col(\"ticker\").str.slice(8, 2).cast(pl.Utf8).alias(\"month\"),\n",
    "    pl.col(\"ticker\").str.slice(10, 2).cast(pl.Utf8).alias(\"day\"),\n",
    "    pl.col(\"ticker\").str.slice(12, 1).alias(\"option_type\"),\n",
    "    (pl.col(\"ticker\").str.slice(13).cast(pl.Float64) / 1000).alias(\"strike_price\")\n",
    "])\n",
    "\n",
    "# Construct expiry date\n",
    "option_df = option_df.with_columns(\n",
    "    (pl.col(\"year\") + \"-\" + pl.col(\"month\") + \"-\" + pl.col(\"day\")).alias(\"expiry_date\")\n",
    ")\n",
    "\n",
    "# Drop intermediate columns used for parsing\n",
    "option_df = option_df.drop([\"year\", \"month\", \"day\"])\n",
    "\n",
    "if \"symbol\" not in df.columns:\n",
    "    df = df.rename({\"ticker\": \"symbol\"})\n",
    "    \n",
    "joined_df = df.join(option_df, on=[\"window_start\", \"symbol\"], how=\"left\")\n",
    "\n",
    "joined_df = joined_df.filter(pl.col(\"strike_price\").is_not_null())\n",
    "price_cols = [\"strike_price\", \"open_right\", \"close_right\", \"high_right\", \"low_right\"]\n",
    "joined_split_df = apply_adjustments(joined_df, splits_df, \"option\", price_cols , [\"volume_right\"])\n",
    "\n",
    "# Define a function to add the time_to_expiry column\n",
    "def add_time_to_expiry(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Ensure expiry_date and transaction_date are in date format\n",
    "    df = df.with_columns([\n",
    "        pl.col('expiry_date').str.strptime(pl.Date, \"%Y-%m-%d\").alias('expiry_date'),\n",
    "        pl.col('transaction_date').cast(pl.Date).alias('transaction_date')\n",
    "    ])\n",
    "    \n",
    "    # Calculate the time_to_expiry in days\n",
    "    df = df.with_columns([\n",
    "        (pl.col('expiry_date') - pl.col('transaction_date')).dt.total_days().alias('time_to_expiry')\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "joined_split_df = add_time_to_expiry(joined_split_df)\n",
    "print(\"Options Data Added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf818a1-f790-46f0-a6af-70591b768be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# # len(df.filter(pl.col(\"transaction_date\") == datetime.date(2024, 3, 14)))\n",
    "# count_per_window_start = joined_df.groupby(\"window_start_est\").agg(pl.count().alias(\"count\"))\n",
    "# count_per_window_start.sort(\"window_start_est\", \"strike_price\")\n",
    "the_cols = ['transaction_date', 'expiry_date', 'time_to_expiry']\n",
    "# joined_split_df.filter(pl.col('time_to_expiry') >= 1)\n",
    "# joined_split_df\n",
    "test_df = joined_split_df.filter(pl.col('time_to_expiry') > 1)\n",
    "test_df.select(the_cols)\n",
    "len(joined_split_df.filter(pl.col('hammer') == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c6978-7e95-4cae-a3a5-0b5c98013527",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### ADD RISK FREE RATE FOR BLACK SCHOLES CALC\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Read the rate DataFrame\n",
    "rate_df = pl.read_csv(\"/Users/brandon/Downloads/daily-treasury-rates.csv\")\n",
    "\n",
    "# Convert the Date column to yyyy-mm-dd format and rename it to rate_date\n",
    "rate_df = rate_df.with_columns([\n",
    "    pl.col('Date').str.strptime(pl.Date, '%m/%d/%Y').alias('rate_date')\n",
    "])\n",
    "\n",
    "# Drop the original Date column\n",
    "rate_df = rate_df.drop('Date')\n",
    "\n",
    "# List of terms and corresponding days\n",
    "terms = [\"1 Mo\", \"2 Mo\", \"3 Mo\", \"4 Mo\", \"6 Mo\", \"1 Yr\", \"2 Yr\", \"3 Yr\", \"5 Yr\"]\n",
    "term_days = [30, 60, 90, 120, 180, 365, 2*365, 3*365, 5*365]\n",
    "\n",
    "# Create a mapping from the terms to the desired column names\n",
    "rename_mapping = {term: f\"{days}_term_rate_period\" for term, days in zip(terms, term_days)}\n",
    "\n",
    "# Rename the columns\n",
    "rate_df = rate_df.rename(rename_mapping)\n",
    "rate_df = rate_df.drop([\"7 Yr\", \"10 Yr\", \"20 Yr\", \"30 Yr\"])\n",
    "\n",
    "# Reorder columns to move rate_date to the first position\n",
    "cols = ['rate_date'] + [col for col in rate_df.columns if col != 'rate_date']\n",
    "rate_df = rate_df.select(cols)\n",
    "# print(rate_df)\n",
    "\n",
    "# Verify columns in rate_df\n",
    "# print(\"Columns in rate_df:\", rate_df.columns)\n",
    "\n",
    "# Verify columns in joined_split_df before the join\n",
    "# print(\"Columns in joined_split_df before the join:\", joined_split_df.columns)\n",
    "\n",
    "# Perform the join\n",
    "joined_split_with_rates_df = joined_split_df.join(rate_df, left_on='transaction_date', right_on='rate_date', how='left')\n",
    "# joined_split_with_rates_df\n",
    "# Verify columns in the resulting DataFrame\n",
    "# print(\"Columns in joined_split_with_rates_df after the join:\", joined_split_with_rates_df.columns)\n",
    "print(\"Rate data added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7fe7a-d374-4506-8b6a-fb83c0173417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# joined_split_with_rates_df.select('rate_date')\n",
    "joined_split_with_rates_df.columns\n",
    "len(joined_split_with_rates_df)\n",
    "# rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275062b-f915-4c2e-8fa8-3a0ab33d284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CALCULATE BLACK-SCHOLES OPTION PRICES AND COMPARE TO ACTUAL\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import polars as pl\n",
    "\n",
    "def black_scholes(S, K, T, r, sigma, option_type='C'):\n",
    "    \"\"\"\n",
    "    Calculate Black-Scholes option price.\n",
    "\n",
    "    Parameters:\n",
    "    S (float): Current stock price\n",
    "    K (float): Strike price\n",
    "    T (float): Time to expiry in years\n",
    "    r (float): Risk-free rate (annualized)\n",
    "    sigma (float): Volatility (annualized)\n",
    "    option_type (str): 'C' for call, 'P' for put\n",
    "\n",
    "    Returns:\n",
    "    float: Theoretical option price\n",
    "    \"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "\n",
    "    if option_type == 'C':\n",
    "        price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    elif option_type == 'P':\n",
    "        price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"option_type must be 'C' for call or 'P' for put\")\n",
    "    \n",
    "    return price\n",
    "\n",
    "def compare_actual_vs_theoretical_prices(df):\n",
    "    \"\"\"\n",
    "    Compare actual option prices with theoretical Black-Scholes prices.\n",
    "\n",
    "    Parameters:\n",
    "    df (pl.DataFrame): DataFrame containing options data\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: DataFrame with added theoretical prices and price differences\n",
    "    \"\"\"\n",
    "    # Ensure necessary columns are present\n",
    "    required_columns = ['close', 'strike_price', 'time_to_expiry', \n",
    "                        '30_term_rate_period', 'hv', 'option_type', 'close_right']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame is missing one or more required columns: {required_columns}\")\n",
    "\n",
    "    # Convert time to expiry to years\n",
    "    df = df.with_columns([\n",
    "        (pl.col('time_to_expiry') / 365).alias('time_to_expiry_years')\n",
    "    ])\n",
    "\n",
    "    # Define the lambda function for calculating theoretical option prices\n",
    "    bs_lambda = lambda row: black_scholes(\n",
    "        S=row['close'],\n",
    "        K=row['strike_price'],\n",
    "        T=row['time_to_expiry_years'],\n",
    "        r=row['30_term_rate_period'],\n",
    "        sigma=row['hv'],\n",
    "        option_type=row['option_type']\n",
    "    )\n",
    "\n",
    "    # Calculate theoretical option prices and add to the DataFrame\n",
    "    df = df.with_columns([\n",
    "        pl.struct([\n",
    "            'close',\n",
    "            'strike_price',\n",
    "            'time_to_expiry_years',\n",
    "            '30_term_rate_period',\n",
    "            'hv',\n",
    "            'option_type'\n",
    "        ]).map_elements(bs_lambda).alias('bs_theoretical_price')\n",
    "    ])\n",
    "\n",
    "    # Calculate the difference between actual and theoretical prices\n",
    "    df = df.with_columns([\n",
    "        (pl.col('close_right') - pl.col('bs_theoretical_price')).alias('price_difference'),\n",
    "        (\n",
    "            pl.when(pl.col('bs_theoretical_price') != 0)\n",
    "            .then((pl.col('close_right') - pl.col('bs_theoretical_price')) / pl.col('bs_theoretical_price'))\n",
    "            .otherwise(None)\n",
    "        ).alias('price_difference_pct')\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    df_with_price_comparison = compare_actual_vs_theoretical_prices(joined_split_with_rates_df)\n",
    "    print(\"Price comparison completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in price comparison: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab3665-70b9-4c28-a5ad-0a39d08f79fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cols = ['close', 'strike_price', 'time_to_expiry', 'risk_free_rate', 'hv', 'option_type', 'close_right']\n",
    "df_with_price_comparison.filter(pl.col(\"bullish_hammer\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca23e0a-0f89-489c-ab40-1b92dad05f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####APPLY TRADING STRATEGIIES\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants for default signal generation\n",
    "DEFAULT_STOP_LOSS_PCT = 0.02\n",
    "DEFAULT_TAKE_PROFIT_PCT = 0.04\n",
    "EXPIRATION_DAYS = 30\n",
    "STRIKE_PRICE_FACTOR = 1.02\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    # Function to process each batch of data\n",
    "    result = generate_all_signals(batch)\n",
    "    logging.info(f\"Processed a batch with {len(batch)} rows.\")\n",
    "    return result\n",
    "\n",
    "def generate_trade_signals(df: pl.DataFrame, pattern: str, trade_signal: str, \n",
    "                           stop_loss_pct: float, take_profit_pct: float) -> pl.DataFrame:\n",
    "    return df.with_columns([\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.lit(trade_signal))\n",
    "        .otherwise(None).alias(f\"{pattern}_trade_signal\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"close_right\"))\n",
    "        .otherwise(None).alias(f\"{pattern}_entry_price\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"close_right\") * (1 - stop_loss_pct))\n",
    "        .otherwise(None).alias(f\"{pattern}_stop_loss\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"close_right\") * (1 + take_profit_pct))\n",
    "        .otherwise(None).alias(f\"{pattern}_take_profit\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"expiry_date\"))  # Use the extracted expiry date\n",
    "        .otherwise(None).alias(f\"{pattern}_expiration_date\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"strike_price\"))  # Use the extracted strike price\n",
    "        .otherwise(None).alias(f\"{pattern}_strike_price\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"option_type\"))  # Use the extracted option type\n",
    "        .otherwise(None).alias(f\"{pattern}_option_type\")\n",
    "    ])\n",
    "\n",
    "def generate_all_signals(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    patterns = [\n",
    "    {\"pattern\": \"bullish_engulfing\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"bearish_engulfing\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"momentum\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"multiple_upper_wicks\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"multiple_lower_wicks\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"bullish_hammer\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"shooting_star\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"bullish_tweezer\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"bearish_tweezer\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"bullish_marubozu\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    {\"pattern\": \"bearish_marubozu\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "]\n",
    "    \n",
    "    for p in patterns:\n",
    "        df = generate_trade_signals(df, p[\"pattern\"], p[\"trade_signal\"], p[\"stop_loss_pct\"], p[\"take_profit_pct\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main(df, batch_size=10000, num_workers=None):\n",
    "    # Split the DataFrame into batches\n",
    "    batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    # Determine the number of workers\n",
    "    if num_workers is None:\n",
    "        num_workers = os.cpu_count()\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        \n",
    "        results = []\n",
    "        for future in as_completed(futures):\n",
    "            logging.info(\"Started processing a future.\")\n",
    "            results.append(future.result())\n",
    "            logging.info(\"Completed processing a future.\")\n",
    "    \n",
    "    # Combine the results back into a single DataFrame\n",
    "    combined_df = pl.concat(results)\n",
    "    logging.info(\"Completed processing all batches.\")\n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "# parallel_df = main(joined_split_with_rates_df, num_workers=8)\n",
    "parallel_df = main(df_with_price_comparison, num_workers=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df4c21-d38a-4345-996c-fb27cbb66773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def generate_trade_signals(df: pl.DataFrame, pattern: str, trade_signal: str, \n",
    "                           stop_loss_pct: float, take_profit_pct: float) -> pl.DataFrame:\n",
    "    # Ensure the pattern column is Boolean\n",
    "    df = df.with_columns(pl.col(pattern).cast(pl.Boolean))\n",
    "\n",
    "    return df.with_columns([\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.lit(trade_signal))\n",
    "        .otherwise(None).alias(f\"{pattern}_trade_signal\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"close_right\"))\n",
    "        .otherwise(None).alias(f\"{pattern}_entry_price\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"close_right\") * (1 - stop_loss_pct))\n",
    "        .otherwise(None).alias(f\"{pattern}_stop_loss\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"close_right\") * (1 + take_profit_pct))\n",
    "        .otherwise(None).alias(f\"{pattern}_take_profit\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"expiry_date\"))  # Use the extracted expiry date\n",
    "        .otherwise(None).alias(f\"{pattern}_expiration_date\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"strike_price\"))  # Use the extracted strike price\n",
    "        .otherwise(None).alias(f\"{pattern}_strike_price\"),\n",
    "        pl.when(pl.col(pattern))\n",
    "        .then(pl.col(\"option_type\"))  # Use the extracted option type\n",
    "        .otherwise(None).alias(f\"{pattern}_option_type\")\n",
    "    ])\n",
    "\n",
    "def generate_all_signals(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    patterns = [\n",
    "        {\"pattern\": \"bullish_engulfing\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"bearish_engulfing\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"momentum\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"multiple_upper_wicks\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"multiple_lower_wicks\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"hammer\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"shooting_star\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"bullish_tweezer\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"bearish_tweezer\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"bullish_marubozu\", \"trade_signal\": \"buy call option\", \"option_type\": \"call\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "        {\"pattern\": \"bearish_marubozu\", \"trade_signal\": \"buy put option\", \"option_type\": \"put\", \"stop_loss_pct\": DEFAULT_STOP_LOSS_PCT, \"take_profit_pct\": DEFAULT_TAKE_PROFIT_PCT},\n",
    "    ]\n",
    "\n",
    "    for p in patterns:\n",
    "        df = generate_trade_signals(df, p[\"pattern\"], p[\"trade_signal\"], p[\"stop_loss_pct\"], p[\"take_profit_pct\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_batch(batch):\n",
    "    # Function to process each batch of data\n",
    "    result = generate_all_signals(batch)\n",
    "    logging.info(f\"Processed a batch with {len(batch)} rows.\")\n",
    "    return result\n",
    "\n",
    "def main(df, batch_size=10000, num_workers=4):\n",
    "    # Split DataFrame into batches\n",
    "    batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(process_batch, batch) for batch in batches]\n",
    "        total_futures = len(futures)\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            logging.info(f\"Started processing future {i + 1}/{total_futures}.\")\n",
    "            results.append(future.result())\n",
    "            logging.info(f\"Completed processing future {i + 1}/{total_futures}.\")\n",
    "\n",
    "    # Combine the results back into a single DataFrame\n",
    "    combined_df = pl.concat(results, how='vertical')\n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "parallel_df = main(df_with_price_comparison, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6b8ac-4f0c-44dc-8261-ce8adf7c5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######NARROW TO SINGLE TRADE(code above executes on all available derivatives for that minute)\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "def select_closest_strike_price(df: pl.DataFrame, pattern: str) -> pl.DataFrame:\n",
    "    # Calculate the absolute difference between the strike price and the asset price\n",
    "    df = df.with_columns(\n",
    "        (pl.col(f'{pattern}_strike_price') - pl.col(f'{pattern}_entry_price')).abs().alias('strike_price_diff')\n",
    "    )\n",
    "    \n",
    "    # Group by the pattern and select the trade with the minimum difference\n",
    "    closest_strike_price_df = (\n",
    "        df.filter(pl.col(f'{pattern}_trade_signal').is_not_null())\n",
    "          .sort(['strike_price_diff'])\n",
    "          .group_by([f'{pattern}_trade_signal', 'window_start_est'], maintain_order=True)\n",
    "          .first()\n",
    "    )\n",
    "    \n",
    "    # Drop the intermediate column used for calculation\n",
    "    closest_strike_price_df = closest_strike_price_df.drop('strike_price_diff')\n",
    "    \n",
    "    return closest_strike_price_df\n",
    "\n",
    "def select_closest_strike_prices_for_all_patterns(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    patterns = [\n",
    "        # \"bullish_engulfing\",\n",
    "        # \"bearish_engulfing\",\n",
    "        # \"momentum\",\n",
    "        # \"multiple_upper_wicks\",\n",
    "        # \"multiple_lower_wicks\",\n",
    "        \"bullish_hammer\",\n",
    "        # \"shooting_star\",\n",
    "        # \"bullish_tweezer\",\n",
    "        # \"bearish_tweezer\",\n",
    "        # \"bullish_marubozu\",\n",
    "        # \"bearish_marubozu\"\n",
    "    ]\n",
    "    \n",
    "    closest_strike_prices_df_list = []\n",
    "    for pattern in patterns:\n",
    "        closest_strike_prices_df = select_closest_strike_price(df, pattern)\n",
    "        closest_strike_prices_df_list.append(closest_strike_prices_df)\n",
    "    \n",
    "    # Concatenate all the dataframes\n",
    "    combined_df = pl.concat(closest_strike_prices_df_list, how='vertical')\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "final_df = select_closest_strike_prices_for_all_patterns(parallel_df)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8684c-5077-4151-8b3f-8acfef18296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########EVALUATE TRADE OUTCOMES\n",
    "\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def find_trade_outcome_vectorized(df: pl.DataFrame, pattern: str) -> pl.DataFrame:\n",
    "    outcomes = [None] * len(df)\n",
    "    exit_dates = [None] * len(df)\n",
    "    exit_timestamps = [None] * len(df)\n",
    "    exit_prices = [None] * len(df)\n",
    "    times_to_exit = [None] * len(df)\n",
    "\n",
    "    transaction_dates = df['transaction_date'].to_list()\n",
    "    window_start_ests = df['window_start_est'].to_list()\n",
    "    option_closes = df['close_right'].to_list()  # Using option close price\n",
    "    pattern_flags = df[pattern].to_list()\n",
    "    stop_losses = df[f\"{pattern}_stop_loss\"].to_list()\n",
    "    take_profits = df[f\"{pattern}_take_profit\"].to_list()\n",
    "    expiration_dates = df[f\"{pattern}_expiration_date\"].to_list()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if pattern_flags[i]:\n",
    "            entry_price = option_closes[i]  # Option entry price\n",
    "            stop_loss = stop_losses[i]\n",
    "            take_profit = take_profits[i]\n",
    "            expiration_date = expiration_dates[i]\n",
    "            entry_timestamp = window_start_ests[i]\n",
    "            \n",
    "            trade_outcome = \"undecided\"\n",
    "            exit_date = None\n",
    "            exit_ts = None\n",
    "            exit_price = None\n",
    "            time_to_exit = None\n",
    "\n",
    "            for j in range(i + 1, len(df)):\n",
    "                shifted_transaction_date = transaction_dates[j]\n",
    "                shifted_exit_ts = window_start_ests[j]\n",
    "                \n",
    "                if shifted_exit_ts <= entry_timestamp:\n",
    "                    continue\n",
    "\n",
    "                if shifted_transaction_date > expiration_date:\n",
    "                    break\n",
    "                \n",
    "                shifted_exit_ts = window_start_ests[j]\n",
    "                current_price = option_closes[j]  # Current option price\n",
    "                \n",
    "                if current_price <= stop_loss:\n",
    "                    trade_outcome = \"loss\"\n",
    "                    exit_date = shifted_transaction_date\n",
    "                    exit_ts = shifted_exit_ts\n",
    "                    exit_price = current_price\n",
    "                    time_to_exit = (exit_ts - entry_timestamp).total_seconds() / 60\n",
    "                    break\n",
    "                if current_price >= take_profit:\n",
    "                    trade_outcome = \"win\"\n",
    "                    exit_date = shifted_transaction_date\n",
    "                    exit_ts = shifted_exit_ts\n",
    "                    exit_price = current_price\n",
    "                    time_to_exit = (exit_ts - entry_timestamp).total_seconds() / 60\n",
    "                    break\n",
    "\n",
    "            if trade_outcome == \"undecided\":\n",
    "                trade_outcome = \"expired\"\n",
    "                exit_date = expiration_date\n",
    "                exit_ts = window_start_ests[j]\n",
    "                exit_price = option_closes[j]  # Option price at expiration\n",
    "                time_to_exit = (exit_ts - entry_timestamp).total_seconds() / 60\n",
    "\n",
    "            outcomes[i] = trade_outcome\n",
    "            exit_dates[i] = exit_date\n",
    "            exit_timestamps[i] = exit_ts\n",
    "            exit_prices[i] = exit_price\n",
    "            times_to_exit[i] = time_to_exit\n",
    "\n",
    "\n",
    "    df = df.with_columns([\n",
    "        pl.Series(f\"{pattern}_outcome\", outcomes),\n",
    "        pl.Series(f\"{pattern}_exit_date\", exit_dates).cast(pl.Date),\n",
    "        pl.Series(f\"{pattern}_exit_ts\", exit_timestamps).cast(pl.Datetime),\n",
    "        pl.Series(f\"{pattern}_exit_price\", exit_prices).cast(pl.Float64),\n",
    "        pl.Series(f\"{pattern}_time_to_exit\", times_to_exit).cast(pl.Float64),\n",
    "    ])\n",
    "\n",
    "    df = df.with_columns([\n",
    "        ((pl.col(f\"{pattern}_exit_price\") - pl.col(f\"{pattern}_entry_price\")) * 100).alias(f\"{pattern}_trade_profit\")\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_all_trade_outcomes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    patterns = [\n",
    "        \"bullish_hammer\",\n",
    "        # Add other patterns here as needed\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        start_time = datetime.now().time()\n",
    "        print(f\"***** {start_time} Starting evaluating {pattern} trade results\")\n",
    "        \n",
    "        df = find_trade_outcome_vectorized(df, pattern)\n",
    "        \n",
    "        end_time = datetime.now().time()\n",
    "        print(f\"***** {end_time} Finished evaluating {pattern} trade results\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "vectorized_outcomes_df = find_all_trade_outcomes(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d29a6-8c11-4441-8849-44790b1dfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_outcomes_df.select(cols)\n",
    "vectorized_outcomes_df.select(cols).filter(\n",
    "   # (pl.col(col) == 'loss') &\n",
    "    (pl.col('bullish_hammer_option_type') == 'C') &\n",
    "    (pl.col('time_to_expiry') >= 8) & \n",
    "    (pl.col('time_to_expiry') <= 60)\n",
    ").sort('window_start_est')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807bc4d-c3f3-45ba-875e-969f7be489ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'bullish_hammer_outcome'\n",
    "cols = [ \n",
    "    'window_start_est',\n",
    "    'transaction_date',\n",
    "    'ticker',\n",
    "    'close',\n",
    "    'close_right',\n",
    "    'time_to_expiry',\n",
    "    'bullish_hammer_trade_signal',\n",
    "    'bullish_hammer_outcome',\n",
    " 'bullish_hammer_entry_price',\n",
    "    'bullish_hammer_exit_price',\n",
    "    'bullish_hammer_trade_profit',\n",
    " 'bullish_hammer_stop_loss',\n",
    " 'bullish_hammer_take_profit',\n",
    "    \n",
    " 'bullish_hammer_time_to_exit',\n",
    " 'bullish_hammer_strike_price',\n",
    "'bullish_hammer_exit_ts',\n",
    " 'bullish_hammer_option_type',\n",
    "'bullish_hammer_expiration_date',\n",
    " 'bullish_hammer_exit_date',\n",
    "\n",
    " \n",
    "    #'hammer_time_to_exit'\n",
    "]\n",
    "# vectorized_outcomes_df = vectorized_outcomes_df.filter(pl.col('time_to_expiry') > 1)\n",
    "# vectorized_outcomes_df = vectorized_outcomes_df.filter(\n",
    "#     (pl.col('time_to_expiry') >= 30) & (pl.col('time_to_expiry') <= 60)\n",
    "# )\n",
    "# vectorized_outcomes_df = vectorized_outcomes_df.filter(\n",
    "#     ((pl.col('strike_price') / pl.col('close')).alias('moneyness')).is_between(0.9, 1.1)\n",
    "# )\n",
    "\n",
    "# # vectorized_outcomes_df.select(cols)\n",
    "vectorized_outcomes_df.select(cols).filter(pl.col(col) == 'win').filter(pl.col('bullish_hammer_option_type') == 'C')\n",
    "vectorized_outcomes_df.filter(pl.col(col) == 'win').filter(pl.col('bullish_hammer_option_type') == 'C')\n",
    "\n",
    "vectorized_outcomes_df.select(cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ba2b0-5cd5-41e2-856e-18e59bbe3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail(10000)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33073c7d-d49b-407e-996e-a82a1166c2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.filter(pl.col(\"bullish_engulfing\") == True).filter(pl.col(\"outcome\") == \"loss\")\n",
    "df.tail(10000)\n",
    "col = 'hammer'\n",
    "df.filter(pl.col(col) == True)\n",
    "# len(df.filter(pl.col(col) == True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bc62e-e355-492c-ae92-19db020de225",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_outcomes_df.select(cols).filter(pl.col(col) == 'expired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab79be-7d58-4e97-b24c-dd56800e2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_df.write_parquet(\"/Users/brandon/Documents/stock_backtest_outcomes.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd83d6-c9a4-4d99-b584-f65c8802ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate win/loss ratio and overall profit\n",
    "def calculate_strategy_performance(df: pl.DataFrame, strategies: list) -> pl.DataFrame:\n",
    "    performance_data = []\n",
    "\n",
    "    for strategy in strategies:\n",
    "        # Filter data for the strategy\n",
    "        strategy_df = df.filter(pl.col(f\"{strategy}_trade_signal\").is_not_null())\n",
    "        \n",
    "        # Calculate wins and losses\n",
    "        wins = strategy_df.filter(pl.col(f\"{strategy}_outcome\") == \"win\").shape[0]\n",
    "        losses = strategy_df.filter(pl.col(f\"{strategy}_outcome\") == \"loss\").shape[0]\n",
    "        total_trades = wins + losses\n",
    "        \n",
    "        # Calculate win/loss ratio\n",
    "        win_loss_ratio = wins / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        # Calculate overall profit\n",
    "        profit = strategy_df.filter(pl.col(f\"{strategy}_outcome\") == \"win\").with_columns(\n",
    "            (pl.col(f\"{strategy}_exit_price\") - pl.col(f\"{strategy}_entry_price\")).alias(f\"{strategy}_profit\")\n",
    "        ).select(pl.col(f\"{strategy}_profit\").sum()).item()\n",
    "        \n",
    "        performance_data.append((strategy, total_trades, win_loss_ratio, profit))\n",
    "\n",
    "    # Create a new DataFrame with performance metrics\n",
    "    performance_df = pl.DataFrame(performance_data, schema=[\"Strategy\", \"Total Trades\", \"Win/Loss Ratio\", \"Overall Profit\"])\n",
    "    return performance_df\n",
    "\n",
    "# List of strategies\n",
    "strategies = [\n",
    "    # \"bullish_engulfing\",\n",
    "    # \"bearish_engulfing\",\n",
    "    # \"momentum\", \n",
    "    # \"multiple_upper_wicks\",\n",
    "    # \"multiple_lower_wicks\",\n",
    "    \"hammer\",\n",
    "    # \"shooting_star\",\n",
    "    # \"bullish_tweezer\", \n",
    "    # \"bearish_tweezer\", \n",
    "    # \"bullish_marubozu\",\n",
    "    # \"bearish_marubozu\"\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "performance_df = calculate_strategy_performance(outcomes_df, strategies)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff2e18-bf10-4dfb-95d1-581ba603eaaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'hammer'\n",
    "# df.filter(pl.col(col) == True)\n",
    "parallel_df.filter(pl.col(col) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baffee5-38f5-4565-8373-122490c5aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['hammer',\n",
    "           'hammer_trade_signal',\n",
    "         'hammer_entry_price',\n",
    "         'hammer_stop_loss',\n",
    "         'hammer_take_profit',\n",
    "         'hammer_expiration_date',\n",
    "         'hammer_strike_price',\n",
    "         'hammer_option_type',\n",
    "         'hammer_outcome',\n",
    "         'hammer_exit_date',\n",
    "         'hammer_exit_ts',\n",
    "         'hammer_exit_price',\n",
    "        ]\n",
    "\n",
    "col = 'hammer'\n",
    "\n",
    "vectorized_outcomes_df.select(columns).filter(pl.col(col) == True)\n",
    "\n",
    "vectorized_outcomes_df.filter(pl.col(col) == 'loss').filter(pl.col('hammer_option_type') == 'C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287afda1-9e88-4ff0-ac7d-cbcf7def31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%history -g 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6425a5-7d79-4ae5-9407-582e486c2c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
